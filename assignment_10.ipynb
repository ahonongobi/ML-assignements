{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You: **Ahonon Gobi Parfait**\n",
    "\n",
    "Teammate: (No one available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tenth lab. Neural networks are more a class of tools than a single tool, though the foundation you built last week should enable you to understand what is going on here without too much trouble.\n",
    "\n",
    "There is relatively little coding this week, which is unfortunate: we are starting to touch topics that require more than a lab's worth of practice to achieve basic proficiency. Rather than overloading you of work, this week we focus a bit more on foundations and give you time to study; then we should hit more interesting and fun applications over the next lectures with Deep Learning and Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 14 points (out of 21 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[1pt]** Explain in English what is the distinctive feature of a residual network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **distinctive feature of a residual network (ResNet)** is the use of skip connections, also known as shortcut connections or residual connections. These connections allow information from earlier layers to bypass several layers and directly connect to deeper layers in the network. By doing so, ResNet addresses the degradation problem encountered in deep neural networks, where adding more layers can lead to decreased performance due to vanishing gradients or the difficulty of training deeper networks. The skip connections in ResNet enable the network to learn residual functions, which are easier to optimize compared to directly learning the underlying mapping. This architecture facilitates the training of very deep networks (hundreds or even thousands of layers) while maintaining or even improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[2pt]** Write the full equation of a network with structure [2, 4, 1] (same as last week), but this time add (i) biases on all neurons, and (ii) self-recurrent connections only on the hidden layer. How many weights does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I would suggest starting from your answer from last week, fixing it based on the solution if you need to and have not already, then add what you need.\n",
    "- To avoid changing the indices of the weights, you can simply call bias weights $b$ rather than $w$, and recurrent connections $r$.\n",
    "- The main thing to remember is: each line has the weights entering one destination neuron, and each column refers to one of the inputs to the layer.\n",
    "- Then for a recurrent network, remember to pass the output of all neurons of the same layer (technically representing the previous-step activations, initialized as `0`s) as inputs to each neuron.\n",
    "\n",
    "The network has three layers:\n",
    "- An input layer (no neurons!) with two elements $(x_1, x_2)$\n",
    "- One hidden layer composed of four neurons $(n_1, n_2, n_3, n_4)$\n",
    "- The output layer with only one neuron $(n_5)$\n",
    "\n",
    "We will need to add biases and recurrencies this time: it could be helpful to describe the inputs/outputs for each layer together with the weight matrix. \n",
    "- $X$ is the network input, same as before\n",
    "- Then come the recurrent connections: all the outputs of the neurons of the hidden layer, technically from the previous time step (initialize as zeros)\n",
    "- Finally the bias input, the constant 1 that will be multiplied by the bias weight\n",
    "- $X_{hid}$ is the actual full input to the hidden (and first) layer: all three above\n",
    "- $W_h$ is the weight matrix for ALL the connections entering the hidden layer in the columns, while the rows group the connections entering each neuron\n",
    "- The output can be written with $n_i$ same as we did last time; the don't forget you will need the bias also for the output layer (but no recursion!)\n",
    "- You can call $X_{out}$ the input to the output (and second) layer, and $W_{out}$ its weight matrix.\n",
    "And do not underestimate the value of a quick sketch on a piece of paper! Or head to [draw.io](https://draw.io) if you want a computer drawing that is easy, quick and professional looking.\n",
    "\n",
    "Remember that the output can be interpreted as one scalar, but is in principle a vector with one element (because having only one output is a special case, normally you need a list of outputs here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first, find out X, Xhid, Whid: \n",
    "\n",
    "$$X=\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$\n",
    "$$ Xhid = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}$$\n",
    "\n",
    "$$\n",
    "W_{in} = \\begin{pmatrix} \n",
    "w_{1} & w_{2} & b_{1} \\\\\n",
    "w_{3} & w_{4} & b_{2} \\\\\n",
    "w_{5} & w_{6} & b_{3} \\\\\n",
    "w_{7} & w_{8} & b_{4} \\\\\n",
    "r_{3} & r_{4} & b_{5} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$$  \n",
    "Whid = \\begin{pmatrix}\n",
    "w_{9} & w_{10} & w_{11} & w_{12} \\\\\n",
    "r_{1} & r_{2} & r_{3} & r_{4} \\\\\n",
    "b_{1} & b_{2} & b_{3} & b_{4} \\\\\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "$$X_{out} = \\begin{pmatrix} n_5 \\end{pmatrix}$$\n",
    "$$W_{out} = \\begin{pmatrix} w_{1} & w_{2} & w_{3} & w_{4} & b \\end{pmatrix}$$\n",
    "\n",
    "$$nhid = \\sigma(Win*X) = \\begin{pmatrix} \\sigma(w_{1}*x_1 + w_{2}*x_2 + b_{1}) \\\\ \\sigma(w_{3}*x_1 + w_{4}*x_2 + b_{2}) \\\\ \\sigma(w_{5}*x_1 + w_{6}*x_2 + b_{3}) \\\\ \\sigma(w_{7}*x_1 + w_{8}*x_2 + b_{4}) \\end{pmatrix}$$\n",
    "\n",
    "$$\n",
    "nout = \\sigma(w_hid*n_hid) \\\\=  \\sigma(w_{9}*n_1 + w_{10}*n_2 + w_{11}*n_3 + w_{12}*n_4 + b)\n",
    "\\\\ = \\sigma(w_{9}*\\sigma(w_{1}*x_1 + w_{2}*x_2 + b_{1}) + w_{10}*\\sigma(w_{3}*x_1 \n",
    "+ w_{4}*x_2 + b_{2}) + w_{11}*\\sigma(w_{5}*x_1 + w_{6}*x_2 + b_{3}) + w_{12}*\\sigma(w_{7}*x_1 + w_{8}*x_2 + b_{4}) + b)\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **[2pt]** A neural network has only one layer of two convolutional neurons with identity activation. Below you will find respective kernels $W_1$ and $W_2$ and input $X$. Activate the network on the input by hand showing all calculation. Assume no padding and state explicitly the expected output size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to understand what you need to explain about your calculations if you actually start doing them :) just mark what you actually input in the calculator, what the calculator returns, and what calculation you are confident skipping.\n",
    "\n",
    "$$\n",
    "W_1 = \n",
    "\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    " 2 &  2 &  2 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix}\n",
    "\\quad,\\quad\n",
    "W_2 = \n",
    "\\begin{pmatrix}\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "2 & 2 & 2 & 2 & 2 \\\\\n",
    "3 & 3 & 3 & 3 & 3 \\\\\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "3 & 3 & 3 & 3 & 3 \\\\\n",
    "2 & 2 & 2 & 2 & 2 \n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the input matrix \\( X \\) of size \\( 5 \\times 5 \\) and the kernels \\( W_1 \\) and \\( W_2 \\) of size \\( 3 \\times 3 \\), the output will be of size \\( (5-3+1) \\times (5-3+1) = 3 \\times 3 \\) for each kernel.  \n",
    "Let's perform the convolution operation for the first kernel \\( W_1 \\) for \\( h \\) at position \\( (0,0) \\): \n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "2 & 2 & 2 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "2 & 2 & 2 \\\\\n",
    "3 & 3 & 3 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 2 + -1 \\times 2 + -1 \\times 2 + 2 \\times 3 + 2 \\times 3 + 2 \\times 3 + -1 \\times 1 + -1 \\times 1 + -1 \\times 1 \\\\\n",
    "&= -2 - 2 - 2 + 6 + 6 + 6 - 1 - 1 - 1 \\\\\n",
    "&= 9\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Now, let's perform the convolution operation for the second kernel \\( W_2 \\) for \\( h \\) at position \\( (0,0) \\):\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "2 & 2 & 2 \\\\\n",
    "3 & 3 & 3 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 2 + 2 \\times 2 + -1 \\times 2 + -1 \\times 3 + 2 \\times 3 + -1 \\times 3 + -1 \\times 1 + 2 \\times 1 + -1 \\times 1 \\\\\n",
    "&= -2 + 4 - 2 - 3 + 6 - 3 - 1 + 2 - 1 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Now let us perform the convolution operation for the first kernel \\( W_1 \\) for \\( h \\) at position \\( (0,1) \\):\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "2 & 2 & 2 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "2 & 2 & 2 \\\\\n",
    "3 & 3 & 3 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 2 + -1 \\times 2 + -1 \\times 2 + 2 \\times 3 + 2 \\times 3 + 2 \\times 3 + -1 \\times 1 + -1 \\times 1 + -1 \\times 1 \\\\\n",
    "&= -2 - 2 - 2 + 6 + 6 + 6 - 1 - 1 - 1 \\\\\n",
    "&= 9\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Now let us perform the convolution operation for the second kernel \\( W_2 \\) for \\( h \\) at position \\( (0,1) \\):\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "2 & 2 & 2 \\\\\n",
    "3 & 3 & 3 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 2 + 2 \\times 2 + -1 \\times 2 + -1 \\times 3 + 2 \\times 3 + -1 \\times 3 + -1 \\times 1 + 2 \\times 1 + -1 \\times 1 \\\\\n",
    "&= -2 + 4 - 2 - 3 + 6 - 3 - 1 + 2 - 1 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Position \\( (0,2) \\) for \\( W_1 \\) and \\( W_2 \\) will be 9 and 0 respectively.\n",
    "\n",
    "Position \\( (1,0) \\) for \\( W_1 \\): \n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "2 & 2 & 2 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "3 & 3 & 3 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "3 & 3 & 3\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 3 + -1 \\times 3 + -1 \\times 3 + 2 \\times 1 + 2 \\times 1 + 2 \\times 1 + -1 \\times 3 + -1 \\times 3 + -1 \\times 3 \\\\\n",
    "&= -3 - 3 - 3 + 2 + 2 + 2 - 3 - 3 - 3 \\\\\n",
    "&= -12\n",
    "\\end{align*}\n",
    "\\]\n",
    "** Same for Position \\( (1,1) \\) and \\( (1,2) \\) for \\( W_1 \\). **\n",
    "\n",
    "Position \\( (1,0) \\) for \\( W_2 \\):\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "3 & 3 & 3 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "3 & 3 & 3\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 3 + 2 \\times 3 + -1 \\times 3 + -1 \\times 1 + 2 \\times 1 + -1 \\times 1 + -1 \\times 3 + 2 \\times 3 + -1 \\times 3 \\\\\n",
    "&= -3 + 6 - 3 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\\]\n",
    "** Same for Position \\( (1,1) \\) and \\( (1,2) \\) for \\( W_2 \\). **\n",
    "\n",
    "Position \\( (2,0) \\) for \\( W_1 \\):\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "2 & 2 & 2 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "3 & 3 & 3 \\\\\n",
    "2 & 2 & 2\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 1 + -1 \\times 1 + -1 \\times 1 + 2 \\times 3 + 2 \\times 3 + 2 \\times 3 + -1 \\times 2 + -1 \\times 2 + -1 \\times 2 \\\\\n",
    "&= -1 - 1 - 1 + 6 + 6 + 6 - 2 - 2 - 2 \\\\\n",
    "&= 9\n",
    "\\end{align*}\n",
    "\\]\n",
    "** Same for Position \\( (2,1) \\) and \\( (2,2) \\) for \\( W_1 \\). **\n",
    "\n",
    "Position \\( (2,0) \\) for \\( W_2 \\):\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "&\\begin{pmatrix}\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1\n",
    "\\end{pmatrix} \\circledast \\begin{pmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "3 & 3 & 3 \\\\\n",
    "2 & 2 & 2\n",
    "\\end{pmatrix} \\\\\n",
    "&= -1 \\times 1 + 2 \\times 1 + -1 \\times 1 + -1 \\times 3 + 2 \\times 3 + -1 \\times 3 + -1 \\times 2 + 2 \\times 2 + -1 \\times 2 \\\\\n",
    "&= -1 + 2 - 1 - 3 + 6 - 3 - 2 + 4 - 2 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\\]\n",
    "** Same for Position $\\( (2,1) \\)$ and $\\( (2,2) \\)$ for $\\( W_2 \\)$. **\n",
    "\n",
    "The expected output size is \\( 3 \\times 3 \\) for each kernel.\n",
    "\n",
    "we have the following matrix for the output of the first kernel $\\( W_1 \\)$:\n",
    "\\[ \\begin{pmatrix} 9 & 9  & 9 \\\\ -12 & -12 & -12 \\\\ 9 & 9 & 9 \\end{pmatrix} \\]\n",
    "\n",
    "and the following matrix for the output of the second kernel $\\( W_2 \\)$:\n",
    "\\[ \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 **[2pt]** Look at the activations of the two neurons from 1.3 and discuss why they are so different. Explain in particular the regularities both in the inputs and in the kernels. Then go one step further and explain, to the best of your understanding, which types of features are detected by the two kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is another open question: as long as you do not write anything wrong, while showing competence and intuition, you will get the points.\n",
    "- Hint: focus on thinking about the _patterns_ that you can see by eye both in the data matrix and in the kernels. Try to go for an intuitive answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activations obtained from the two neurons reflect the distinct patterns they are sensitive to, as determined by their respective kernels $ W_1 $ and $ W_2 $. For the activations produced by $ W_1 $, we observe high values (9) in regions where smooth transitions or gradients are present in the input matrix $ X $. This suggests that $ W_1 $ detects features related to smooth transitions or gradients. Conversely, $ W_2 $ produces activations with values of 0 throughout, indicating a lack of response to the input patterns. This aligns with the interpretation that $ W_2 $ is more sensitive to edges or abrupt changes in the input, which might not be prevalent in $ X $. This interpretation is supported by the structure of the kernels themselves: $ W_1 $ is configured to detect gradual changes due to its uniform weights, while $ W_2 $ is designed to respond to sharp transitions with alternating weights. Therefore, the differences in activations between the two neurons can be attributed to the distinct feature detection capabilities encoded in their respective kernels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 **[1pt]** Activate a $3x3$ max pooling layer on the outputs of the two convolutions from your answer to question 1.3. Assume no padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform max pooling on the outputs of the two convolutions from the previous question. The max pooling operation involves dividing the input into non-overlapping regions of a specified size (in this case, $3 \\times 3$) and selecting the maximum value within each region. The output size will be reduced by a factor of the pooling size, which is $3 \\times 3$ in this case.\n",
    "\n",
    "For the output of the first kernel $ W_1 $:\n",
    "\\[ \\begin{pmatrix} 9 & 9  & 9 \\\\ -12 & -12 & -12 \\\\ 9 & 9 & 9 \\end{pmatrix} \\]\n",
    "\n",
    "The max pooling operation will yield the following output:\n",
    "\\[ \\begin{pmatrix} 9 \\end{pmatrix} \\]\n",
    "\n",
    "For the output of the second kernel $ W_2 $:\n",
    "\\[ \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\]\n",
    "\n",
    "The max pooling operation will yield the following output:\n",
    "\\[ \\begin{pmatrix} 0 \\end{pmatrix} \\]\n",
    "\n",
    "The output of the max pooling layer for both kernels is a single value, which represents the maximum activation value within the $3 \\times 3$ region of the input matrix. This operation helps to downsample the feature maps and retain the most salient features while reducing the spatial dimensions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 **[1pt]** Explain in one sentence what is an autoencoder. Why do autoencoders have an hourglass shape? Could you design an autoencoder with a different shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a type of neural network architecture designed to learn efficient representations of input data by training the network to reconstruct its own inputs, typically consisting of an encoder and a decoder; **autoencoders have an hourglass** shape because they first compress the input data into a lower-dimensional latent space (encoder) and then reconstruct the original input from this compressed representation (decoder), resembling an hourglass in structure; **while the hourglass shape is common** due to its effectiveness in learning hierarchical representations, alternative shapes could be designed depending on specific requirements, such as expanding or contracting structures to accommodate different data distributions or task objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[3pt]** Below is last week's implementation of a neural network augmented into a fully-connected RNN with bias connections. Fix it by writing the missing code as marked by `?`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unless otherwise stated, a RNN has fully-connected self-recurrent connections on each layer.\n",
    "- You should know exactly which connections to add if you answered the RNN question in the fundamentals.\n",
    "- For the bias: remember that you need all elements in `state` to be longer by one element: put actual `1`s in these last positions at initialization, then never touch them again.\n",
    "- Recurrencies: you need to make space in the input to each layer for its own output. I typically sort them as [input, recurrencies, bias], but order is not important: consistency is. Make sure all your sizes are correct.\n",
    "- When calculating the size of an input now you need to use `struct` twice: once for the size of the layer entering (something like `struct[nlay]`) and once for the size of the output that goes back as input in the recursion (hence `struct[nlay+1]`). HINT: to make the pairs for each layer execute and understand the following: `zip(struct, struct[1:])`\n",
    "- When you activate a layer remember to copy the activation to both (i) its output and (ii) its input, at the correct indices.\n",
    "- It's easier to compute the size of each input beforehand, then use it to make the `state` list. Then for each weight matrix you can take the number of rows from the structure (as before), and the number of inputs from the `state` sizes.  Don't worry about duplicating the activation in the layer's input, it's actually faster because you have a ready numpy array rather than composing at activation.\n",
    "- Remember to initialize the recurrent output to `0`. Simplest way is to initialize `state` using `np.zeros()` instead of `np.empty()`. Then set the last value of each `state` element to `1` for the bias.\n",
    "- I used myself `import IPython; IPython.embed()` heavily to get this to work. You can also call (once!) `%pdb` to drop in the debugger on error. Keep calm and check the dimensions.\n",
    "- The layer activation function should change because you are saving the activation to two locations (and at specific indices, not direct substitution like before), but the network activation function should change just marginally (add indices to insert input in state)\n",
    "- To _convolve_ with stride 1 and no padding a window of size two on a 1D list (take a pair at a time, advance by one) in Python you can use `zip(lst, lst[1:])`.\n",
    "- This question only refers to neural networks, not learning algorithm, so leave backpropagation out and do not worry about unrolling the network.\n",
    "- Again activate it on a simple input to verify everything is works. The input should be exactly the same as last week's (as the network architecture).\n",
    "- Think: how many weights do you expect to have? Remember that you have 3 matrices (for the 3 layers of neurons), in each the number of rows is unchanged (because you have one per each neuron) but the inputs now are not only connections from the previous layer, you also have recursion (take the output of this layer as its own input) and bias (constant 1 appended to the inputs).\n",
    "<!-- Secret hint: my tally is (4+3+1)*3 + (3+4+1)*4 + (4+3+1)*3 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T22:55:11.264516Z",
     "start_time": "2024-05-08T22:55:11.257383Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self, struct):\n",
    "        # These are basic, copy+paste from FFNN\n",
    "        self.struct = struct\n",
    "        self.nlayers = len(struct)\n",
    "        self.nins,*self.nhids,self.nouts = self.struct\n",
    "        self.sigma = lambda x: 1/(1+np.exp(-x))\n",
    "        \n",
    "        # Each `state` is an input for next layer: it now includes rec and bias\n",
    "        state_sizes = [inp + rec + 1  for inp, rec in zip(self.struct, self.struct[1:])]\n",
    "        # Notice the `zip` above ends when the second list reaches the end (1 shorter)\n",
    "        # Last `state` is only the output of the last layer (no rec/b)\n",
    "        state_sizes += [self.struct[-1]]\n",
    "        # We can now build the state of the network\n",
    "        self.state = [np.zeros(size) for size in state_sizes]\n",
    "        \n",
    "        # We will need to access inputs and recurrencies by index: the following helps\n",
    "        self.inp_idxs = [range(0, inp) for inp in self.struct[:-1]]\n",
    "        self.rec_idxs = [range(inp, inp + rec) for inp, rec in zip(self.struct[:-1], self.struct[1:])]\n",
    "        # Finally, fix the bias input in the last position of all input `state`s\n",
    "        # Just set and forget, and no need for indices because we won't access it again\n",
    "        for s in self.state: s[-1] = 1\n",
    "        \n",
    "        # The `state` sizes now correspond to the row lengths (ncols) for the weight matrices\n",
    "        self.wsizes = [[inp, rec] for inp, rec in zip(self.struct[1:], self.struct)]\n",
    "        # Finally: weight initialization. Bad practice to hardcode this, but ok here\n",
    "        self.weights = [np.random.normal(size=ws) for ws in self.wsizes]\n",
    "\n",
    "    # The layer activation is unchanged: sigma(W.dot(X)) -- only W and X (=state) differ\n",
    "    def act_layer(self, nlay):\n",
    "        #return self.sigma(self.weights[nlay].dot(self.state[nlay]))\n",
    "         return self.sigma(self.weights[nlay].dot(self.state[nlay]))\n",
    "\n",
    "    # The network activation only writes the act twice this time: output & rec-input\n",
    "    def act_net(self, inp):\n",
    "        assert len(inp) == self.nins, f\"got input `{inp}`, expected np.array of length `{self.nins}`\"\n",
    "        self.state[0][self.inp_idxs[0]] = inp\n",
    "        for nlay in range(self.nlayers-1):\n",
    "            act = self.act_layer(nlay)\n",
    "            # This time the layer activation goes in two places:\n",
    "            # - In the input indices of the output of this layer / input to next\n",
    "            self.state[nlay+1][self.inp_idxs[nlay]] = act\n",
    "            # - In the recurrent indices of the input to this layer\n",
    "            self.state[nlay+1][self.rec_idxs[nlay]] = act\n",
    "        return self.state[-1]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,4) and (10,) not aligned: 4 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_92013/1963819694.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# We expect the activation to change this time upon multiple calls on the same input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# This is because the `state` of the RNN is maintained in the recurrent connections\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"activation 1:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact_net\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"activation 2:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact_net\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"activation 3:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact_net\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_92013/632246048.py\u001B[0m in \u001B[0;36mact_net\u001B[0;34m(self, inp)\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minp_idxs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mnlay\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnlayers\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m             \u001B[0mact\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mact_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnlay\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     41\u001B[0m             \u001B[0;31m# This time the layer activation goes in two places:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# - In the input indices of the output of this layer / input to next\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_92013/632246048.py\u001B[0m in \u001B[0;36mact_layer\u001B[0;34m(self, nlay)\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mact_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnlay\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m         \u001B[0;31m#return self.sigma(self.weights[nlay].dot(self.state[nlay]))\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 33\u001B[0;31m          \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msigma\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweights\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnlay\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnlay\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     34\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0;31m# The network activation only writes the act twice this time: output & rec-input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: shapes (5,4) and (10,) not aligned: 4 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "struct = [4,5,4,3]\n",
    "inputs = [3,2,4,3]\n",
    "net = RecurrentNeuralNetwork(struct)\n",
    "# We expect the activation to change this time upon multiple calls on the same input\n",
    "# This is because the `state` of the RNN is maintained in the recurrent connections\n",
    "print(\"activation 1:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 2:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 3:\", net.act_net(np.array(inputs)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T22:55:11.349908Z",
     "start_time": "2024-05-08T22:55:11.265301Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self, struct):\n",
    "        self.struct = struct\n",
    "        self.nlayers = len(struct)\n",
    "        self.nins,*self.nhids,self.nouts = self.struct\n",
    "        self.sigma = lambda x: 1/(1+np.exp(-x))\n",
    "\n",
    "        # Adjust the sizes of the state and weights arrays\n",
    "        state_sizes = [inp + nhid + 1 for inp, nhid in zip(self.struct[:-1], self.nhids)]\n",
    "        state_sizes += [self.nouts]\n",
    "        self.state = [np.zeros(size) for size in state_sizes]\n",
    "\n",
    "        self.inp_idxs = [range(0, inp) for inp in self.struct[:-1]]\n",
    "        self.rec_idxs = [range(inp, inp + nhid) for inp, nhid in zip(self.struct[:-1], self.nhids)]\n",
    "        for s in self.state: s[-1] = 1\n",
    "\n",
    "        self.wsizes = [[nout, inp + nhid + 1] for nout, inp, nhid in zip(self.struct[1:], self.struct[:-1], self.nhids)]\n",
    "        self.weights = [np.random.normal(size=ws) for ws in self.wsizes]\n",
    "\n",
    "    def act_layer(self, nlay):\n",
    "        return self.sigma(self.weights[nlay].dot(self.state[nlay]))\n",
    "\n",
    "    def act_net(self, inp):\n",
    "        assert len(inp) == self.nins, f\"got input `{inp}`, expected np.array of length `{self.nins}`\"\n",
    "        self.state[0][self.inp_idxs[0]] = inp\n",
    "        for nlay in range(self.nlayers-1):\n",
    "            act = self.act_layer(nlay)\n",
    "            self.state[nlay+1][self.inp_idxs[nlay]] = act\n",
    "            self.state[nlay+1][self.rec_idxs[nlay]] = act\n",
    "        return self.state[-1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = [4,5,4,3]\n",
    "inputs = [3,2,4,3]\n",
    "net = RecurrentNeuralNetwork(struct)\n",
    "# We expect the activation to change this time upon multiple calls on the same input\n",
    "# This is because the `state` of the RNN is maintained in the recurrent connections\n",
    "print(\"activation 1:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 2:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 3:\", net.act_net(np.array(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[2pt]** Write a Python function for 2D convolution, then run it on a randomly generated input matrix and show the output.\n",
    "\n",
    "- You need to write a function that takes a 2D input and a function to convolve as parameters, then convolves the function over the inputs to produce the output.\n",
    "- The window size is not specified: you can use a 3x3 to keep it simple since you saw that in the examples.\n",
    "- The function to convolve is not specified: no need for it to be a neural network, something as simple as `sum` or a quick lambda calling the numpy `x.sum()` would work perfectly well. Remember that the important part is that it should take a high-dimensional (3x3?) input and output only one value.\n",
    "- The size of the input matrix is not specified: with a 3x3 mask we could go as small as 5x5 with no padding and that would still show that the convolution works.\n",
    "- Then remember: the neural networks are just other functions than `sum`, but behave exactly the same way. Convolving a neural network only allows you to learn the function rather than hardcoding it, but the convolution process is independent.\n",
    "- Answer the question until the end: show that you know how to create a matrix of random numbers, and of the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T23:08:25.284130Z",
     "start_time": "2024-05-08T23:08:25.267312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix:\n",
      "[[0.86504947 0.05551674 0.60113627 0.85538797 0.50563441]\n",
      " [0.44613408 0.82953874 0.72775907 0.69985604 0.87777094]\n",
      " [0.25030068 0.13805881 0.51727254 0.82769675 0.72712334]\n",
      " [0.42104016 0.5515829  0.39557917 0.84695226 0.96366649]\n",
      " [0.5739887  0.13590139 0.67583035 0.880991   0.43862352]]\n",
      "\n",
      "Output Matrix after 2D Convolution:\n",
      "[[4.43076642 5.25222294 6.33963733]\n",
      " [4.27726616 5.53429628 6.58367659]\n",
      " [3.65955471 4.96986517 6.27373541]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convolve_2d(input_matrix, conv_function):\n",
    "    # Get input matrix dimensions\n",
    "    height, width = input_matrix.shape\n",
    "\n",
    "    # Initialize output matrix\n",
    "    output_matrix = np.zeros((height - 2, width - 2))\n",
    "\n",
    "    # Iterate over the input matrix\n",
    "    for i in range(1, height - 1):\n",
    "        for j in range(1, width - 1):\n",
    "            # Extract 3x3 window from input matrix\n",
    "            window = input_matrix[i-1:i+2, j-1:j+2]\n",
    "\n",
    "            # Apply convolution function to window and store result in output matrix\n",
    "            output_matrix[i-1, j-1] = conv_function(window)\n",
    "\n",
    "    return output_matrix\n",
    "\n",
    "# Example of a simple convolution function: sum of the window\n",
    "simple_conv_function = lambda window: np.sum(window)\n",
    "\n",
    "# Generate a random 5x5 input matrix\n",
    "input_matrix = np.random.rand(5, 5)\n",
    "\n",
    "# Perform 2D convolution\n",
    "output_matrix = convolve_2d(input_matrix, simple_conv_function)\n",
    "\n",
    "print(\"Input Matrix:\")\n",
    "print(input_matrix)\n",
    "print(\"\\nOutput Matrix after 2D Convolution:\")\n",
    "print(output_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Handwritten digit recognition with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning, we need to cross a gap in exercise complexity. On one hand you are ready to understand the inner work of a DL library like Keras, on the other asking you for such a task on top of today's lecture is too much even for this course :) So let's leave the creative part for next weeks, where we will see some more advanced applications anyway, and focus today on what we learned and on a new skill: how to justify your code.\n",
    "\n",
    "Below is a tutorial from the Keras website on convolutional networks [[source]](https://keras.io/examples/vision/mnist_convnet/). It uses the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a standard dataset for handwritten character recognition. Keras offers you a backend to automatically download the dataset, similarly to what we did so far with Seaborn and Iris.\n",
    "\n",
    "The code should work as is (did you `pipenv install tensorflow keras`?), but take a while to run. Read it line by line, really study and understand it, feel free to change it so that it runs in few seconds if you wish to play with it; then answer the questions below.\n",
    "\n",
    "NOTE: you don't need to run the code to answer any of the questions below. If you used PyTorch at the last lecture, this is your chance to try out Keras. If you cannot (looking at you M1 users), you can use Colab for this assignment, or replace the Keras tutorial with a PyTorch equivalent [such as this](https://pythonguides.com/pytorch-mnist/). This only affects 4.3, while 4.1 and 4.2 should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T23:00:05.940373Z",
     "start_time": "2024-05-08T22:57:40.653121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 00:57:40.955791: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: gobi-ROG-Zephyrus-M15-GU502LW-GU502LW\n",
      "2024-05-09 00:57:40.955812: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: gobi-ROG-Zephyrus-M15-GU502LW-GU502LW\n",
      "2024-05-09 00:57:40.955905: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2024-05-09 00:57:40.955940: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 535.171.4\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"sequential\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (\u001B[38;5;33mConv2D\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m26\u001B[0m, \u001B[38;5;34m26\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │           \u001B[38;5;34m320\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (\u001B[38;5;33mMaxPooling2D\u001B[0m)    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m13\u001B[0m, \u001B[38;5;34m13\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m11\u001B[0m, \u001B[38;5;34m11\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │        \u001B[38;5;34m18,496\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m, \u001B[38;5;34m5\u001B[0m, \u001B[38;5;34m64\u001B[0m)       │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001B[38;5;33mFlatten\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1600\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1600\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │        \u001B[38;5;34m16,010\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,010</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m34,826\u001B[0m (136.04 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m34,826\u001B[0m (136.04 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 00:57:41.134355: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 169344000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 23ms/step - accuracy: 0.7578 - loss: 0.7928 - val_accuracy: 0.9783 - val_loss: 0.0826\n",
      "Epoch 2/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9614 - loss: 0.1252 - val_accuracy: 0.9830 - val_loss: 0.0598\n",
      "Epoch 3/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9718 - loss: 0.0914 - val_accuracy: 0.9857 - val_loss: 0.0482\n",
      "Epoch 4/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9771 - loss: 0.0738 - val_accuracy: 0.9888 - val_loss: 0.0433\n",
      "Epoch 5/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9806 - loss: 0.0637 - val_accuracy: 0.9908 - val_loss: 0.0381\n",
      "Epoch 6/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 23ms/step - accuracy: 0.9823 - loss: 0.0583 - val_accuracy: 0.9902 - val_loss: 0.0352\n",
      "Epoch 7/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9825 - loss: 0.0555 - val_accuracy: 0.9918 - val_loss: 0.0346\n",
      "Epoch 8/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 23ms/step - accuracy: 0.9839 - loss: 0.0507 - val_accuracy: 0.9913 - val_loss: 0.0343\n",
      "Epoch 9/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 23ms/step - accuracy: 0.9858 - loss: 0.0430 - val_accuracy: 0.9912 - val_loss: 0.0344\n",
      "Epoch 10/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9862 - loss: 0.0431 - val_accuracy: 0.9918 - val_loss: 0.0318\n",
      "Epoch 11/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9878 - loss: 0.0363 - val_accuracy: 0.9913 - val_loss: 0.0302\n",
      "Epoch 12/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9879 - loss: 0.0390 - val_accuracy: 0.9922 - val_loss: 0.0278\n",
      "Epoch 13/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9883 - loss: 0.0360 - val_accuracy: 0.9907 - val_loss: 0.0284\n",
      "Epoch 14/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 22ms/step - accuracy: 0.9888 - loss: 0.0343 - val_accuracy: 0.9925 - val_loss: 0.0282\n",
      "Epoch 15/15\n",
      "\u001B[1m422/422\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 24ms/step - accuracy: 0.9887 - loss: 0.0352 - val_accuracy: 0.9918 - val_loss: 0.0291\n",
      "Test loss: 0.026795532554388046\n",
      "Test accuracy: 0.9912999868392944\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: Simple MNIST convnet\n",
    "Author: [fchollet](https://twitter.com/fchollet)\n",
    "Date created: 2015/06/19\n",
    "Last modified: 2020/04/21\n",
    "Description: A simple convnet that achieves ~99% test accuracy on MNIST.\n",
    "SOURCE: https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the data\n",
    "\"\"\"\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "## Build the model\n",
    "\"\"\"\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Train the model\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "## Evaluate the trained model\n",
    "\"\"\"\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 **[1pt]** Data plotting: plot the first 9 images in MNIST using a 6x6 subplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see what the MNIST looks like. I showed how to use subplots in a recent solution -- do you remember where it was? You'll need the ability to search quickly for what you need to complete the exam in time. Try timing how long it takes you to answer this question (no seriously challenge your teammate on who solves this the quickest and feel free to brag about it in your solution below).\n",
    "- Add the label on the title to see how the numbers are represented: do you see the connection to last week's `species`?\n",
    "- After you obtain the axis from `plt.subplots()`, you can print a 2D image using `ax[?,?].imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x720 with 9 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArUAAALICAYAAABhHAheAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA140lEQVR4nO3ce5SXVb0/8M9wEUEFQTkKdUxNRZNGEAgQBPOGeQHvSgpqJiwssU558KTH4xXLshWCmOYtzZVpJmDlreOInFSCjNYhxAsZqAN4A0TE4TLf3x+twy+zZ49957qH12st1tL1ns/z7C+6h7cb3BWlUqkUAACQsTbNvQAAAKgvpRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUtuCjRkzJu6///4mnwX+OfYq5MFebd2U2iZw6KGHxtNPP93cyyj0i1/8Ivbbb7/o27fvlh9z585t7mVBk2vpezUi4s4774whQ4ZEv3794j/+4z9iw4YNzb0kaHI57NX/M3bs2OjVq1ds2rSpuZfS6im1REREnz594g9/+MOWHwMHDmzuJQF/Z86cOXHLLbfEnXfeGU888US89tprccMNNzT3soACs2bNis2bNzf3MrYaSm0zWrNmTYwfPz4GDRoUAwYMiPHjx8eKFSs+9DXLli2Lk08+Ofr16xcTJkyI1atXb8kWLFgQp59+evTv3z9GjhzpdBUaSUvZqzNmzIiTTz459t577+jSpUucf/758eCDD9bno0Gr0lL2akTE2rVr48Ybb4yLLrqo7Gfwz1Fqm1FtbW2ceOKJUVVVFVVVVdGhQ4e48sorP/Q1M2bMiMmTJ8ecOXOiXbt2cfXVV0dExMqVK2P8+PExYcKE+N3vfheTJk2KiRMnxjvvvPOR91RXV0f//v2jurq6cC3PP/98DBw4MEaMGBE33nij3yaBv9FS9upLL70U++6775a/79WrV7z11luxatWqBvy0kK+WslcjIr7//e/H6NGjY+edd27YD0khpbYZde3aNUaMGBEdO3aM7bffPiZMmBDz5s370NeMGjUq9tlnn+jUqVNceOGF8cgjj8TmzZtj5syZMWzYsBg+fHi0adMmhgwZEr17947Zs2d/5D09e/aM+fPnR8+ePf/hOgYMGBAPPfRQPPPMM3HDDTfEr371q7jtttsa5TNDjlrKXn3//fdj++233/L3O+ywQ0RErFu3rgE/LeSrpezV//3f/43nnnsuzjzzzEb5nPxj7Zp7AVuz9evXx7XXXhtz5syJNWvWRMRff3HavHlztG3bNiIievToseXre/bsGRs3boxVq1ZFdXV1PPLII1FVVbUl37RpU1l/FvZf//Vft/x1r1694itf+UrcdtttMX78+HI/GrQqLWWvdurUKd57770tf/9/f73ddtuV9bmgtWkJe7W2tjauuOKKuOSSS6JdOzWrKfnZbka33357vPLKK3HfffdF9+7d4/nnn4/jjz8+SqXSlq9Zvnz5h/66ffv20bVr1+jRo0eMGjVqy2+bNKSKiooPrQG2di1lr+69997xwgsvxNFHHx0REYsXL46dd945unbtWu9nQ2vQEvbqe++9FwsXLoyvf/3rERFb/kex4cOHx5QpU6J///71ej7F/PGDJrJx48aoqanZ8mPTpk2xbt266NChQ3Tu3DlWr14d06ZN+8jcrFmz4uWXX47169fHlClTYsSIEdG2bdsYOXJkVFVVxZw5c2Lz5s1RU1MTc+fO/cgfiP84Zs+eHW+99VZERCxZsiSmT58ehx12WL0/M+SoJe/VUaNGxc9//vN4+eWXY82aNXHTTTfFCSec0BAfG7LTUvfqDjvsEHPmzIkZM2bEjBkz4pZbbomIv16fWVlZ2SCfnX9MqW0i48aNi8rKyi0/pk6dGmeddVbU1NTEoEGD4rTTTouDDz74I3OjRo2Kiy++OIYMGRIbNmyISy65JCL++tsn06dPj5tvvjkGDx4cw4cPj9tuuy1qa2s/8ozq6uro27dv4R9of/bZZ2PkyJHRp0+fGDduXBxxxBH+6AFbrZa8V4cNGxZf/vKXY+zYsfH5z38+PvGJT8TEiRMb9icAMtFS92pFRUV07959y49u3bpFRMROO+0U22yzTQP/LPC3Kkp+nxkAgMw5qQUAIHtKLQAA2VNqAQDInlILAED2kvfUVlRUNNU6oNVqiv8X016F+rNXIQ9Fe9VJLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkL12zb0AgK1dv379kvlXv/rVwmzs2LHJ2bvuuqswmzp1anL2ueeeS+YALYmTWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2asolUqlwrCioinXstVq27ZtYdalS5dGe2/qmqBOnTolZ3v16lWYfeUrX0nOfu973yvMRo8enZz94IMPCrNvf/vbydkrrrgimTeWxBZrMPZqy9enT5/C7IknnkjOdu7cuYFX81dr1qxJ5jvttFOjvLelslfJ1WGHHVaY3XPPPcnZ4cOHF2YvvPBC2WtqTEV71UktAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGSvXXMvoCXZbbfdCrNtttkmOXvQQQcVZkOHDk3O7rjjjoXZSSedlJxtLq+99lphdsMNNyRnTzjhhMJs7dq1ydk//vGPhdns2bOTs9CYPve5zyXzBx54oDCr6z7q1P2pde2ZDRs2FGZ13UM7aNCgwuy5554r+700vWHDhhVmdf178OCDDzb0cmhgAwYMKMzmzZvXhCtpXk5qAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkb6u60qtPnz7J/IknnijM6rpyp7Wpra1N5pdeemlh9t577yVn77nnnsJs+fLlydlVq1YVZi+88EJyFurSqVOnZH7ggQcWZj/5yU+Ssz169ChrTXV56aWXkvl1111XmN17773J2d/+9reFWep7QETEtddem8xpWoccckhhtvfeeydnXenV/Nq0SZ9B7rHHHoXZpz71qeRsRUVFWWtqiZzUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2tqp7apctW5bM33777cKspd5TO3fu3MJs9erVydnPf/7zhdmGDRuSs3fffXcyhxzdfPPNyXz06NFNtJKPL3V3bkTE9ttvX5jNnj07OZu627SysjI5S8syduzYwuyZZ55pwpVQjrruuT7vvPMKs7ru0F68eHFZa2qJnNQCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMjeVnWl1zvvvJPML7roosLs2GOPTc7+4Q9/KMxuuOGG9MISFixYkMyPOOKIwmzdunXJ2f33378wu/DCC5OzkKt+/foVZsccc0xytqKiouz3pq7Peuihh5Kz3/ve9wqz6urq5Gzqe9OqVauSs4ceemhhVp+fC5pemzbOsHJ26623lj370ksvNeBKWjb/lgMAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9raqe2rrMmPGjMLsiSeeSM6uXbu2MDvggAOSs+eee25hlrqfMqLuu2hT/vSnPxVm48aNK/u50Jz69OmTzB9//PHCrHPnzsnZUqlUmD388MPJ2dGjRxdmw4cPT85eeumlhVld91e++eabhdkf//jH5GxtbW1hVtedvgceeGBh9txzzyVn+edVVlYm81122aWJVkJj6NKlS9mzqe95rY2TWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2XOl18f07rvvlj27Zs2asmfPO++8ZP6zn/2sMEtdxwM522effQqziy66KDmbuhrnrbfeSs4uX768MPvxj3+cnH3vvfcKs1/96lfJ2bry5tCxY8dk/o1vfKMwO+OMMxp6OVu9o48+OpnX9c+L5pe6dm2PPfYo+7mvv/562bO5cVILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlzT20TuPzyy5N5v379CrPhw4cnZw8//PDC7LHHHkvOQkvVoUOHZP69732vMKvrvs61a9cWZmPHjk3Ozp8/vzBzD+iH7bbbbs29hK1Kr169yp7905/+1IAroVyp72upO2wjIl588cXCLPU9r7VxUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHuu9GoC69atS+bnnXdeYfbcc88lZ3/0ox8VZlVVVcnZ1PVEN954Y3K2VColc6iPvn37JvO6ru1KGTVqVGE2e/bssp8LuZo3b15zLyEbnTt3TuZHHXVUYXbmmWcmZ4888siy1hQRcdVVVxVmq1evLvu5uXFSCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZc09tC7BkyZLC7Oyzz07O3nHHHYXZmDFjkrOpfLvttkvO3nXXXYXZ8uXLk7NQl+9///vJvKKiojCr665Zd9F+fG3aFJ971NbWNuFKaEzdunVrlvcecMAByTy1zw8//PDk7Cc/+cnCbJtttknOnnHGGYVZak9ERKxfv74wmzt3bnK2pqamMGvXLl3Xfv/73yfzrYWTWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2XOlVwv34IMPJvOXXnqpMKvrWqTDDjusMJs8eXJy9lOf+lRhds011yRnX3/99WTO1uHYY48tzPr06ZOcLZVKhdmsWbPKXRJ/J3VtV+qfQUTEggULGng1pKSukopI//P64Q9/mJz91re+Vdaa6lJZWZnMU1d6bdq0KTn7/vvvF2aLFi1Kzt5+++2F2fz585OzqSsDV65cmZx97bXXCrOOHTsmZxcvXpzMtxZOagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge+6pzdzChQsLs1NPPTU5e9xxxxVmd9xxR3J2/Pjxhdnee++dnD3iiCOSOVuH1L2L22yzTXL2jTfeKMx+9rOflb2m1qhDhw6F2eWXX172c5944olk/h//8R9lP5t/3vnnn5/Mly5dWpgddNBBDb2cj2XZsmXJfMaMGYXZ888/n5x99tlny1lSoxo3blwy7969e2H25z//uaGX0yo5qQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD1XerViq1evTuZ33313YXbrrbcmZ9u1K/5XZ9iwYcnZQw45pDB78sknk7MQEVFTU1OYLV++vAlX0vxSV3ZFRFx66aWF2UUXXZScfe211wqz66+/Pjn73nvvJXOa1ne+853mXsJW77DDDit79oEHHmjAlbReTmoBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHvuqc1cZWVlYXbyyScnZwcMGFCYpe6hrcuiRYuS+VNPPVX2syEiYtasWc29hCbVp0+fwqyuu2ZPO+20wmzmzJnJ2ZNOOimZA03jwQcfbO4lZMFJLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7LnSqwXo1atXYfbVr341OXviiScWZrvuumvZa6rL5s2bC7Ply5cnZ2traxt6OWSooqKirCwi4vjjjy/MLrzwwnKX1Gy+/vWvJ/P//M//LMy6dOmSnL3nnnsKs7Fjx6YXBpARJ7UAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD331DaQ1J2wo0ePTs6m7qLdfffdy11SvcyfPz+ZX3PNNYXZrFmzGno5tEKlUqmsLCK932644Ybk7O23316Yvf3228nZQYMGFWZjxoxJzh5wwAGF2Sc/+cnk7LJlywqzRx99NDk7ffr0ZA60DKn7uffZZ5/k7LPPPtvQy8mSk1oAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlzpdff2GWXXQqzz3zmM8nZadOmFWb77rtv2Wuqj7lz5ybz7373u4XZzJkzk7O1tbVlrQkaQtu2bQuz888/Pzl70kknFWbvvvtucnbvvfdOL6xMTz/9dDKvqqoqzC677LKGXg7QDFJXGbZp4wzy4/CzBABA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZa3X31Hbr1q0wu/nmm5Ozffr0Kcz23HPPcpdUL3XdX3n99dcXZo8++mhydv369WWtCRrCM888U5jNmzcvOTtgwICy37vrrrsWZqm7quvy9ttvJ/N77723MLvwwgvLfi/Q+g0ePDiZ33nnnU2zkBbOSS0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOy1yCu9Bg4cWJhddNFFydnPfe5zhdknPvGJstdUH++//34yv+GGGwqzyZMnJ2fXrVtX1pqgub322muF2YknnpicHT9+fGF26aWXlr2mukyZMqUwu+mmm5KzL7/8ckMvB2hFKioqmnsJ2XNSCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZa5H31J5wwgllZfW1aNGiwuyXv/xlcnbTpk2F2fXXX5+cXb16dTKHrc3y5cuT+eWXX15WBtBcHn744WR+yimnNNFKWi8ntQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsldRKpVKhWFFRVOuBVqlxBZrMPYq1J+9Cnko2qtOagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkr6JUKpWaexEAAFAfTmoBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1LZgY8aMifvvv7/JZ4F/jr0KebBXWzeltgkceuih8fTTTzf3Mgq9+OKLce6558bAgQOjV69ezb0caDYtfa9u2LAhJk+eHEOHDo0BAwbE5ZdfHhs3bmzuZUGTa+l79cEHH4wTTzwxDjzwwBg2bFhcd911sWnTpuZeVqun1BLt2rWLo446Kq655prmXgqQcMstt8TChQvjl7/8ZTz66KOxaNGiuOmmm5p7WcDfWb9+fXzrW9+KZ599Nu6///549tln4/bbb2/uZbV6Sm0zWrNmTYwfPz4GDRoUAwYMiPHjx8eKFSs+9DXLli2Lk08+Ofr16xcTJkyI1atXb8kWLFgQp59+evTv3z9GjhwZc+fOLWsde+65Z5xyyimx99571+fjQKvVUvbqE088EWPGjIkdd9wxunXrFmPGjIkHHnigPh8NWpWWsle/+MUvRv/+/WObbbaJXXbZJY477rh47rnn6vPR+BiU2mZUW1sbJ554YlRVVUVVVVV06NAhrrzyyg99zYwZM2Ly5MkxZ86caNeuXVx99dUREbFy5coYP358TJgwIX73u9/FpEmTYuLEifHOO+985D3V1dXRv3//qK6ubpLPBa1NS9qrpVLpQ3+9YsWKWLt2bQN9UshbS9qrf2vevHmx11571f8DkqTUNqOuXbvGiBEjomPHjrH99tvHhAkTYt68eR/6mlGjRsU+++wTnTp1igsvvDAeeeSR2Lx5c8ycOTOGDRsWw4cPjzZt2sSQIUOid+/eMXv27I+8p2fPnjF//vzo2bNnU300aFVayl49+OCD46677op33nkn3nzzzbj77rsj4q+/1Qm0nL36tx544IFYuHBhfOlLX2qwz8k/1q65F7A1W79+fVx77bUxZ86cWLNmTURErFu3LjZv3hxt27aNiIgePXps+fqePXvGxo0bY9WqVVFdXR2PPPJIVFVVbck3bdoUAwcObNoPAVuBlrJXJ0yYEGvXro1Ro0bFNttsE6eeemo8//zzsdNOO9XzE0Lr0FL26v/5zW9+E9dff33ccccd0a1bt7Kfw8ej1Daj22+/PV555ZW47777onv37vH888/H8ccf/6HfXly+fPmH/rp9+/bRtWvX6NGjR4waNWrLb5sAjael7NVtt902LrvssrjssssiIuJnP/tZ7L///lt+sYatXUvZqxERTz31VFx66aVxyy23uFmoifjjB01k48aNUVNTs+XHpk2bYt26ddGhQ4fo3LlzrF69OqZNm/aRuVmzZsXLL78c69evjylTpsSIESOibdu2MXLkyKiqqoo5c+bE5s2bo6amJubOnfuRPxD/cZRKpaipqdlyNVBNTU1s2LCh3p8ZctSS9+rKlStj5cqVUSqVYsGCBTF9+vS44IILGuJjQ3Za8l595pln4qKLLoqpU6dGZWVlQ3xcPgaltomMGzcuKisrt/yYOnVqnHXWWVFTUxODBg2K0047LQ4++OCPzI0aNSouvvjiGDJkSGzYsCEuueSSiPjrb59Mnz49br755hg8eHAMHz48brvttqitrf3IM6qrq6Nv376Ff6D99ddfj8rKyjjmmGMiIqKysjKOOuqoBvz0kI+WvFeXLVsWo0ePjj59+sSkSZPiG9/4RgwdOrRhfwIgEy15r06fPj3Wrl0b48aNi759+0bfvn3jy1/+csP+BPARFaW/PZMHAIAMOakFACB7Si0AANlTagEAyJ5SCwBA9pL31FZUVDTVOqDVaor/F9NehfqzVyEPRXvVSS0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPbaNfcCyNOll15amF1xxRXJ2TZtiv9b6pBDDknOzp49O5kDQGPZYYcdkvn2229fmB1zzDHJ2e7duxdm3//+95OzNTU1yXxr4aQWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALLnnlr+obPPPjuZT5o0qTCrra0t+72lUqnsWQCoy+67757MU7++DR48ODnbu3fvcpZUpx49eiTziRMnNsp7c+OkFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9lzpxT/0qU99Kplvu+22TbQSaDkGDhxYmJ155pnJ2eHDhxdm+++/f9lr+uY3v5nMq6urC7OhQ4cmZ3/yk58UZnPnzk0vDBrRvvvum8y/9rWvFWZnnHFGcrZjx46FWUVFRXL21VdfLczWrl2bnN1vv/0Ks1NPPTU5O3369MJs8eLFydnWxEktAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGTPPbVbscMPP7wwu+CCC8p+bl134h177LGF2cqVK8t+L9TXaaedlsynTJlSmO28887J2dT9lk8++WRytnv37oXZd7/73eRsSl13bqbee/rpp5f9XoiI6NKlSzL/zne+U5jVtVd32GGHstZUl5deeimZjxgxojBr3759cjb1a2dd31/qyrcWTmoBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGTPlV6t2NChQ5P5HXfcUZjVddVKSl1XDC1durTsZ0Nd2rVLf1vr379/YfajH/0oOdupU6fC7KmnnkrOXnXVVYXZ//zP/yRnO3ToUJjdd999ydkjjzwymafMnz+/7FmoywknnJDMv/zlLzfRSj5syZIlhdkRRxyRnH311VcLs7322qvsNfHxOKkFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOy5p7YVO+uss5J5z549y372k08+WZjdddddZT8X6uvMM89M5rfeemvZz3788ccLs9NOOy05++6775b93tSz63MP7WuvvZbMf/zjH5f9bKjLKaec0mjP/stf/lKYzZs3Lzk7adKkwix1D21d9ttvv7Jn+Xic1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ4rvTK38847F2Zf+tKXkrO1tbWF2erVq5OzV199dTKHxnTVVVcVZt/61reSs6VSqTCbPn16cvbSSy8tzOpzZVddLrnkkkZ57sSJE5P5m2++2SjvhYiI8847L5mPGzeuMHvssceSsy+//HJh9sYbb6QX1kh22WWXZnnv1sRJLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkzz21Ldzuu++ezB944IFGee/UqVOTeVVVVaO8FyIiLrvssmSeuot2w4YNydlHH320MJs0aVJydv369ck8Zdttty3MjjzyyOTsbrvtVphVVFQkZ1N3Ss+cOTM5C42puro6mV9++eVNs5AmMnjw4OZeQqvnpBYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPZc6dXCHXXUUcm8srKy7Gf/93//d2E2ZcqUsp8LH8eOO+5YmJ1//vnJ2VKpVJilruyKiDj++OOTebn22muvZH7PPfcUZv369Sv7vT//+c+T+XXXXVf2s6E1mjhxYjLfbrvtGuW9n/3sZ8ueffrpp5P5M888U/azWxMntQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQvYpS4sLHioqKplzLVit1b+add96ZnE3dp1fXvXannnpqYbZy5crkLB9f6k7VhpLjXv2Xf/mXwqy6urrs5+65557J/IMPPijMzjnnnOTsyJEjC7PevXsnZ7fffvvCrK5/R1L5iSeemJx96KGHkjn/n73asnTq1Kkw+8xnPpOc/a//+q/C7Oijjy57TW3apM8Ca2try3526vveIYcckpxdsmRJ2e/NUdFedVILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB77Zp7AVuD3XffPZk/8MADjfLeP//5z8nctV00pw0bNhRmb775ZnK2e/fuhdkrr7ySnG2sa5vquobs3XffLcx69OiRnH3rrbcKM1d20ZK1b9++MOvbt29yNvVrY117Zv369YVZXXv1mWeeKcyOOuqo5GzqGrK6tGtXXMnqurpvypQphVnqe21r46QWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALLnntomMGnSpGReW1vbKO/99re/3SjPhYawevXqwuz4449Pzv7yl78szLp165acXbJkSWE2c+bM5Oydd95ZmL3zzjvJ2Xvvvbcwq+vOzdQsNKdtttkmmafudf3FL35R9nuvuOKKZP7EE08UZr/97W+Ts6nvIannRkT07t07maek7t++9tprk7PLli0rzGbMmJGcrampSeY5cVILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7rvRqIH369CnMjjzyyEZ7b+oKohdeeKHR3guNae7cuck8dfVNcxk2bFgyHz58eGFW17V+f/7zn8taEzSE9u3bF2Z1Xa110UUXlf3ehx9+uDCbOnVqcjZ1ZWBd3z9+/etfF2af/exnk7MbNmwozK677rrkbOo6sFGjRiVn77nnnsLsN7/5TXL2O9/5TmG2atWq5GzKggULyp4tl5NaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMheRalUKhWGFRVNuZasvfHGG4VZ165dy37us88+m8y/8IUvFGbvvfde2e+l4SS2WIOxV5vfiBEjknnq7su6/h3p0aNHYfbmm2+mF8bHtrXu1bZt2ybza665pjD75je/mZxdt25dYXbxxRcnZ++9997CrK77U/v371+YTZs2rezZl19+OTk7YcKEwqyqqio527lz58LsoIMOSs6eccYZhdnIkSOTs9ttt10yT3n11VcLsz322KPs59alaK86qQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD1XejWQzZs3F2a1tbVlP3fs2LHJ/Kc//WnZz6ZpbK3XBPFhqe8RrvRqGbbWvZq6hioiYurUqYXZ+++/n5wdN25cYfbYY48lZwcOHFiYnXPOOcnZ1HWXHTt2TM5eeeWVhdkdd9yRnE1dcdVcRo8ency/+MUvlv3sr3/964VZXdef1YcrvQAAaLWUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2XNP7cdU1910Z599dmFWn3tq99xzz2S+dOnSsp9N09ha777c2owYMSKZ//rXvy7M3FPbMmyte3X58uXJvHv37oVZTU1Ncnbx4sWF2XbbbZec3WuvvZJ5uS6//PJkfu211xZmqfumaTruqQUAoNVSagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOy1a+4FtCR9+vQpzA4//PDkbOrarg0bNiRnb7zxxsJs5cqVyVmgZajr+j1oqVasWJHMU1d6dejQITl7wAEHlLWmiPQ1eE899VRydsaMGYXZX/7yl+Ssa7vy5aQWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALLnntq/seOOOxZmu+66a9nPff3115P5N7/5zbKfDbQMc+bMSeZt2hSfIaTuuYbGNmzYsGR+/PHHF2YHHnhgcvaNN94ozG6//fbk7KpVqwqzuu5/Z+vkpBYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPZc6QXQABYuXJjMX3rppcJszz33TM5++tOfLszefPPN9MKgDmvXrk3md999d1kZNDUntQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPffU/o3FixcXZk8//XRydujQoQ29HKAVmTx5cmF26623JmevueaawuyCCy5Izi5atCi9MIBWwkktAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsVZRKpVJhWFHRlGuBVimxxRqMvdryde7cuTC77777krOHH354YfaLX/wiOXvOOecUZuvWrUvObm3sVchD0V51UgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2XNPLTQyd19Sl9QdthER11xzTWE2YcKE5GxlZWVhtmjRovTCtjL2KuTBPbUAALRaSi0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9V3pBI3NNEOTBXoU8uNILAIBWS6kFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJC95D21AACQAye1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2pbsDFjxsT999/f5LPAP8dehTzYq62bUtsEDj300Hj66aebexmFXnzxxTj33HNj4MCB0atXr+ZeDjSblr5Xf/WrX8WIESOiX79+MXjw4Jg0aVK89957zb0saHItfa/6dbV5KLVEu3bt4qijjoprrrmmuZcCJBx44IHx05/+NH7/+9/Hb37zm9i0aVP84Ac/aO5lAX/Hr6vNQ6ltRmvWrInx48fHoEGDYsCAATF+/PhYsWLFh75m2bJlcfLJJ0e/fv1iwoQJsXr16i3ZggUL4vTTT4/+/fvHyJEjY+7cuWWtY88994xTTjkl9t577/p8HGi1Wspe7dGjR3Tr1m3L37dt2zaWLl1a1rOgNWope9Wvq81DqW1GtbW1ceKJJ0ZVVVVUVVVFhw4d4sorr/zQ18yYMSMmT54cc+bMiXbt2sXVV18dERErV66M8ePHx4QJE+J3v/tdTJo0KSZOnBjvvPPOR95TXV0d/fv3j+rq6ib5XNDatKS9On/+/OjXr18ceOCB8dhjj8VZZ53VsB8WMtaS9ipNT6ltRl27do0RI0ZEx44dY/vtt48JEybEvHnzPvQ1o0aNin322Sc6deoUF154YTzyyCOxefPmmDlzZgwbNiyGDx8ebdq0iSFDhkTv3r1j9uzZH3lPz549Y/78+dGzZ8+m+mjQqrSkvdq/f//4/e9/H0899VSce+658YlPfKLBPy/kqiXtVZpeu+ZewNZs/fr1ce2118acOXNizZo1ERGxbt262Lx5c7Rt2zYi/vrbjf+nZ8+esXHjxli1alVUV1fHI488ElVVVVvyTZs2xcCBA5v2Q8BWoCXu1V122SUOPvjg+Ld/+7d48MEH6/UsaC1a4l6l6Si1zej222+PV155Je67777o3r17PP/883H88cdHqVTa8jXLly//0F+3b98+unbtGj169IhRo0Zt+W0ToPG01L26adOmWLZsWYM/F3LVUvcqTcMfP2giGzdujJqami0/Nm3aFOvWrYsOHTpE586dY/Xq1TFt2rSPzM2aNStefvnlWL9+fUyZMiVGjBgRbdu2jZEjR0ZVVVXMmTMnNm/eHDU1NTF37tyP/IH4j6NUKkVNTU1s3LgxIiJqampiw4YN9f7MkKOWvFdnzZoV1dXVUSqV4vXXX48f/OAHMXjw4Ib42JCdlrxX/braPJTaJjJu3LiorKzc8mPq1Klx1llnRU1NTQwaNChOO+20OPjggz8yN2rUqLj44otjyJAhsWHDhrjkkksi4q+/fTJ9+vS4+eabY/DgwTF8+PC47bbbora29iPPqK6ujr59+xb+gfbXX389Kisr45hjjomIiMrKyjjqqKMa8NNDPlryXl2yZEmcfvrp0bdv3xg9enTssccecdVVVzXsTwBkoiXvVb+uNo+K0t+eyQMAQIac1AIAkD2lFgCA7Cm1AABkT6kFACB7yXtqKyoqmmod0Go1xf+Laa9C/dmrkIeiveqkFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyF675l4AEVOmTCnMJk6cmJxduHBhYXbssccmZ5cuXZpeGABAJpzUAgCQPaUWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDIniu9msDuu++ezM8888zCrLa2Njm73377FWb77rtvctaVXvBh++yzTzJv3759YTZs2LDk7PTp0wuzuvZ5c5k5c2ZhdvrppydnN2zY0NDLgY8ttVcPOuig5OzkyZMLsyFDhpS9Jhqfk1oAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ57apvAm2++mcyfeuqpwmzkyJENvRxo1fbff/9kfvbZZxdmp5xySnK2TZvic4CePXsmZ1N30ZZKpeRsc0l9//nhD3+YnP3a175WmL377rvlLgk+li5duhRmVVVVydkVK1YUZrvuumvZszQ+J7UAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2lFoAALLnSq8msG7dumS+dOnSJloJtH7XXnttMj/66KObaCWt29ixY5P5bbfdVpj99re/bejlQINJXdvlSq+WzUktAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsKbUAAGTPPbVNYMcdd0zmBxxwQNMsBLYCjz/+eDKvzz21b7zxRmGWupc1IqJNm+IzhNra2rLXdNBBByXz4cOHl/1s2BpVVFQ09xIok5NaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZc6VXE+jUqVMy32233RrlvQMGDEjmixcvLsyWLl3a0MuBJnHTTTcl8xkzZpT97I0bNxZmK1asKPu59dG5c+dkvnDhwsKsZ8+eZb+3rp/H+fPnl/1saE6lUqkw23bbbZtwJfyznNQCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPbcU9sEqqurk/mdd95ZmF1++eVlv7eu2dWrVxdm06ZNK/u90Jw2bdqUzF999dUmWknTGDFiRDLv2rVro7z3tddeS+Y1NTWN8l5oTv3790/mzz77bBOthH/ESS0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOy50qsFuOqqqwqz+lzpBbQOp59+emF23nnnJWc7duzY0MuJiIjLLrusUZ4LDSF1td+aNWuSs126dCnMPv3pT5e9Jhqfk1oAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ57alu4Nm3S/91RW1vbRCsB6uOMM84ozC6++OLk7F577VWYtW/fvuw11WXBggWF2caNGxvtvVBfq1evLszmzJmTnD322GMbeDU0FSe1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9pRaAACy50qvFq6uK7tKpVITrQTysPvuuyfzMWPGFGaHH354A6/m/xs6dGhh1pj7+N133y3M6rpK7Ne//nVhtn79+rLXBNAYnNQCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPbcUwtkp3fv3oXZrFmzkrO77bZbQy+nRZszZ05hdssttzThSiB/O+20U3MvgQQntQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsudKL6BVqaioqFfeWNq0KT5DqK2tbbT3HnvssYXZF77wheTsww8/3NDLgayNHDmyuZdAgpNaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMiee2pbuNTdlhH1u99y2LBhhdm0adPKfi40toULFxZmhxxySHL2zDPPLMweffTR5OwHH3yQzBvLueeeW5hdcMEFTbgSyF9VVVUyT93tTMvmpBYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPYqSqVSqTCsqGjKtfAPbN68OZkn/vHVS2VlZTJftGhRo7y3NWqsf0Z/y15t3bp06VKYvf3222U/97jjjkvmDz/8cNnPzpG9unU46aSTkvn9999fmK1fvz45+5nPfKYwW7p0aXphfGxFe9VJLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkr11zL4C0H/7wh8l8/PjxjfLecePGJfOvfe1rjfJe4KNGjBjR3EuAVmPTpk1lz9Z1z3CHDh3Kfjb156QWAIDsKbUAAGRPqQUAIHtKLQAA2VNqAQDInlILAED2XOnVwi1evLi5lwCNon379oXZkUcemZx94oknCrP169eXvabmcs455yTzKVOmNNFKoPWbOXNmMk/9urvvvvsmZ1PXXZ5//vnJWerPSS0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZK+iVCqVCsOKiqZcC2V48cUXC7NPf/rTZT+3TZv0f+/stddehdmSJUvKfm9rlNhiDaYl7tWhQ4cm80suuaQwO+KII5Kze+yxR2H26quvphfWSLp165bMjz766MJs6tSpydkddtihrDVFpO/tHTlyZHK2qqqq7PfmaGvdq3zYD37wg8Ksrjuld9lll8Lsgw8+KHdJ/J2iveqkFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9to19wKonz/96U+F2Z577ln2c2tra8uehYiIadOmJfPevXuX/ex///d/L8zWrl1b9nPro65ryA488MDCrD5XST355JPJ/KabbirMtrYru6C+6tqrGzZsaKKV8I84qQUAIHtKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7LmnNnO33HJLYXbcccc14Uqg6UyYMKG5l9Cg3njjjWT+0EMPFWYXXnhhcvaDDz4oa03AR3Xu3DmZjxo1qjB78MEHG3o5/B0ntQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsudKr8wtWrSoMHv++eeTs/vtt19DLwe2OPvss5P5BRdcUJidddZZDbyahrFkyZLC7P3330/OzpkzpzBLXc0XEbFw4cL0woAGc+qppxZmNTU1ydm6ft2lcTmpBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQPaUWAIDsVZRKpVJhWFHRlGuBVimxxRpMjnu1Q4cOhVldd9xeffXVhVnXrl2TszNmzCjMHn/88eTszJkzC7MVK1YkZ2n57FUiIu69997CrK773UeOHFmYLV26tOw18WFFe9VJLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7LnSCxqZa4IgD/Yq5MGVXgAAtFpKLQAA2VNqAQDInlILAED2lFoAALKn1AIAkD2lFgCA7Cm1AABkT6kFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJA9pRYAgOwptQAAZE+pBQAge0otAADZU2oBAMieUgsAQPaUWgAAsqfUAgCQvYpSqVRq7kUAAEB9OKkFACB7Si0AANlTagEAyJ5SCwBA9pRaAACyp9QCAJC9/weODH/KzHtsbAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (_, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Create a 6x6 subplot grid\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "\n",
    "# Plot the first 9 images\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax = axes[i, j]\n",
    "        image_index = i * 3 + j\n",
    "        ax.imshow(x_train[image_index], cmap='gray')\n",
    "        ax.set_title(f\"Label: {y_train[image_index]}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T23:02:08.366261Z",
     "start_time": "2024-05-08T23:02:07.238812Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 **[4pt]** Explain the following lines in the Keras MNIST Tutorial code (in English): 27, 48, 51, 52, 57, 67/77, 68/78, 71\n",
    "\n",
    "- To answer this question you need to show complete competence, as if you wrote this code yourself and you were asked to explain your choices at an oral exam.\n",
    "- For each of the lines mentioned, check the code provided and explain it thoroughly\n",
    "- For each variable, explain its meaning, its use, and the choice of value assigned\n",
    "- For each function call, explain what it does, the meaning of all parameters, and the choices of all values.\n",
    "- Reading the code like \"assign 12 to variable `epochs`\" will not constitute an acceptable answer.\n",
    "- Reading the code like \"creates a new Sequential\" is also not acceptable: check the documentation for `Sequential`, understand what the call does, and present your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Line 27:**\n",
    "```python\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "```\n",
    "Explanation:\n",
    "- This line loads the MNIST dataset from the Keras library.\n",
    "- The MNIST dataset is a collection of handwritten digits (0-9) commonly used for training and testing machine learning models.\n",
    "- `keras.datasets.mnist.load_data()` is a function call that retrieves the MNIST dataset from the Keras library.\n",
    "- The dataset is split into two parts: training data `(x_train, y_train)` and testing data `(x_test, y_test)`.\n",
    "- `x_train` contains the images of handwritten digits used for training the model, while `y_train` contains the corresponding labels (the actual digit each image represents).\n",
    "- Similarly, `x_test` contains the images for testing the trained model, and `y_test` contains their corresponding labels.\n",
    "\n",
    "In summary, line 27 loads the MNIST dataset and splits it into training and testing sets, with images and their corresponding labels for both sets. This dataset will be used to train and evaluate the convolutional neural network model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Line 48 initializes a Sequential model in Keras:**\n",
    "\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- In Keras, a Sequential model is a linear stack of layers.\n",
    "- This line creates an instance of the Sequential model, which will be used to build the convolutional neural network (CNN) for classifying handwritten digits in the MNIST dataset.\n",
    "- The Sequential model allows us to create neural networks layer by layer in a sequential manner.\n",
    "- The parentheses `()` are used to instantiate the Sequential model.\n",
    "- This line initializes an empty Sequential model without any layers added yet.\n",
    "\n",
    "In summary, line 48 initializes an empty Sequential model, which will be further built by adding layers to define the architecture of the CNN for the MNIST classification task.\n",
    "\n",
    "\n",
    "**Line 51 adds a 2D convolutional layer to the Sequential model:**\n",
    "\n",
    "```python\n",
    "layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- `layers.Conv2D()` is a function call that adds a 2D convolutional layer to the Sequential model.\n",
    "- This layer is responsible for performing convolution operations on the input data.\n",
    "- The first parameter `32` specifies the number of filters (or kernels) in the convolutional layer. Each filter detects different features in the input data.\n",
    "- `kernel_size=(3, 3)` specifies the size of the convolutional kernel. In this case, it is a 3x3 kernel, meaning the convolutional operation will be applied in a 3x3 window over the input data.\n",
    "- `activation=\"relu\"` sets the activation function for the layer to Rectified Linear Unit (ReLU). ReLU is a commonly used activation function in convolutional neural networks (CNNs) because of its simplicity and effectiveness in introducing non-linearity to the network.\n",
    "\n",
    "In summary, line 51 adds a 2D convolutional layer to the Sequential model with 32 filters, a 3x3 kernel size, and ReLU activation. This layer is essential for extracting features from the input images during the training process.\n",
    "\n",
    "\n",
    "**Line 52 adds a MaxPooling2D layer to the Sequential model:**\n",
    "\n",
    "```python\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- `layers.MaxPooling2D()` is a function call that adds a 2D max pooling layer to the Sequential model.\n",
    "- Max pooling is a downsampling operation commonly used in convolutional neural networks (CNNs) to reduce the spatial dimensions of the feature maps and extract the most important features.\n",
    "- The `pool_size=(2, 2)` parameter specifies the size of the pooling window. In this case, it is a 2x2 window, meaning the maximum value within each 2x2 window of the input feature maps will be retained, and the rest will be discarded.\n",
    "\n",
    "\n",
    "In summary, line 52 adds a MaxPooling2D layer to the Sequential model with a 2x2 pooling window. This layer helps reduce the spatial dimensions of the feature maps produced by the convolutional layers, making the model more efficient and robust to variations in the input data.\n",
    "\n",
    "\n",
    "**Line 57 adds a fully connected Dense layer to the Sequential model:**\n",
    "\n",
    "```python\n",
    "layers.Dense(num_classes, activation=\"softmax\"),\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "- `layers.Dense()` is a function call that adds a fully connected dense layer to the Sequential model.\n",
    "- Dense layers are also known as fully connected layers, where each neuron in the layer is connected to every neuron in the previous layer.\n",
    "- The first parameter `num_classes` specifies the number of neurons (or units) in the dense layer. In this case, it is set to the number of classes in the classification task, which is 10 for the MNIST dataset (digits 0-9).\n",
    "- `activation=\"softmax\"` sets the activation function for the layer to Softmax. Softmax activation is commonly used in the output layer of classification models to convert the raw output scores into probability distributions over the different classes. It ensures that the output values are normalized and sum up to 1.\n",
    "\n",
    "In summary, line 57 adds a fully connected Dense layer to the Sequential model with a Softmax activation function. This layer is responsible for producing the final output probabilities for each class in the classification task.\n",
    "\n",
    "\n",
    "**Lines 67 and 77 set the batch size for training the model.**\n",
    "\n",
    "- Line 67 assigns the value `128` to the variable `batch_size`, which represents the number of samples processed at once during training.\n",
    "- Line 77 uses the `batch_size` parameter in the `model.fit()` function call to specify the batch size for training. By passing the variable `batch_size` as the parameter value, it ensures consistency and flexibility in adjusting the batch size.\n",
    "\n",
    "\n",
    "**Lines 68 and 78 are related to specifying the number of epochs for training the model using the `model.fit()` function.**\n",
    "\n",
    "Explanation:\n",
    "- Line 68 assigns the value `15` to the variable `epochs`, which represents the number of epochs, i.e., the number of times the entire training dataset is passed forward and backward through the neural network during training.\n",
    "- Line 78, when calling the `model.fit()` function, the `epochs` parameter is set to the value of the variable `epochs` defined earlier. By passing the variable `epochs` as the value for the `epochs` parameter, it specifies the number of training epochs for the model.\n",
    "- Training a neural network for multiple epochs allows it to learn from the training data progressively and refine its weights to improve performance.\n",
    "- Setting an appropriate number of epochs is essential for training deep learning models. Too few epochs may result in underfitting, where the model fails to capture the underlying patterns in the data, while too many epochs may lead to overfitting, where the model learns to memorize the training data and performs poorly on unseen data.\n",
    "- By specifying the number of epochs in both lines 68 and 78, it ensures that the model undergoes training for a fixed number of epochs, allowing it to converge to an optimal solution without overfitting or underfitting.\n",
    "\n",
    "\n",
    "**Line 71 specifies the loss function used during model compilation, particularly for categorical classification tasks.**\n",
    "\n",
    "Explanation:\n",
    "- In line 71, the variable `loss` is assigned the string value `\"categorical_crossentropy\"`. This indicates that the categorical cross-entropy loss function will be used to compute the loss during training. \n",
    "- Categorical cross-entropy is a commonly used loss function for multi-class classification problems where each input sample belongs to one of several classes. It measures the dissimilarity between the true class distributions and the predicted class distributions. For each sample, it computes the cross-entropy loss between the true one-hot encoded class labels and the predicted probabilities output by the model.\n",
    "- When compiling the model in line 71, the `loss` parameter of the `model.compile()` function is set to the value of the variable `loss`, which is `\"categorical_crossentropy\"`. This ensures that during training, the model optimizes its weights to minimize the categorical cross-entropy loss.\n",
    "- Choosing an appropriate loss function is crucial for training deep learning models. The categorical cross-entropy loss is suitable for multi-class classification tasks and is commonly used alongside softmax activation functions in the output layer.\n",
    "- By specifying the loss function as `\"categorical_crossentropy\"` in line 71, it ensures that the model is trained to minimize the discrepancy between the predicted class probabilities and the true class labels, leading to improved accuracy on the classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 **[2pt]** Run Keras MNIST code, tweaking it as needed if it takes too long on your machine. Plot the model's accuracy and loss over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is almost for free since you did the same visualization last week, but you need to get the code to run first.\n",
    "- Also you may want to make sure your changes include setting 'accuracy' and the `history` variable, or at the end of the run you could end up with still nothing to show :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T23:02:25.199096Z",
     "start_time": "2024-05-08T23:02:24.940096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, metric in enumerate(['accuracy', 'loss']):\n",
    "    plt.figure(idx)\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_' + metric])\n",
    "    plt.title('Model ' + metric)\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the end of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
    "\n",
    "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Edit the Keras MNIST code to use a simple RNN, then cheat by passing all images of a class in a sequence (careful with batch size). Reset the network between classes. RNNs will recognize that you expect a constant output per each sequence, decide which output with the first few images, then just saturate the right neurons using the recurrent connections to generate a constant output regardless of the input. You can verify this by then testing the network on a sequence of elements from a constant class, followed by one (or more) elements from another class: they will likely be misclassified. All intelligent learning picks up on shortcuts whenever available, here is a famous example (check the full paper): [husky vs. wolf](https://www.researchgate.net/figure/A-husky-on-the-left-is-confused-with-a-wolf-because-the-pixels-on-the-right_fig1_329277474). Notice that getting a \"simple\" RNN in Keras is not straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of this lecture + exercise you should _own_ neural networks. It does not mean that you know everything about them, but you know enough to understand any resource on the topic, and actually understand how these things work better than most people who just use Keras/Pytorch on a daily basis (unfortunately)."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
