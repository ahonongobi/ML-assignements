{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You: **Ahonon Gobi Parfait**\n",
    "\n",
    "Teammate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the seventh lab. After learning about SVMs last week, we finally introduce the _kernel trick_ and make them capable of tackling nonlinear data. We also introduced more generally the concept of _function mapping_ and learned a bit about word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 16 points (out of 24 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Function Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[3pt]** Give an _original_ example for each of the following concepts (i.e. not one that you find in the slides!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mapping from an example data type to a decision space\n",
    "2. Inverse mapping\n",
    "3. Mapping from the example data to two destination feature spaces\n",
    "4. Mapping from the two feature spaces above to a decision space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example (here is what we presented in the slides):\n",
    "\n",
    "1. Given a picture, decide if it represents an apple or an orange. Original space: \n",
    "$64\\times64\\times3$ images; destination space: $\\{$apple, orange$\\}$.\n",
    "2. Given the label apple, map it to a $64\\times64\\times3$ picture of the fruit.\n",
    "3. Map $\\Phi_1$ goes from the picture to an estimate of average color; map $\\Phi_2$ goes from the picture to a fruit width measured in pixels.\n",
    "4. Map $\\Phi_3$ goes from the two features of estimated color and fruit width, to the decision space of classifying the fruit as apple or orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Mapping from an example data type to a decision space:**\n",
    "   -  Given a set of health metrics (such as heart rate, blood pressure, and cholesterol level), decide if a patient has a high risk of heart disease. Original space: Health metrics; destination space: {High risk, Low risk}.\n",
    "\n",
    "2. **Inverse mapping:**\n",
    "   -  Given a diagnosis of \"High risk\" for heart disease, map it back to the corresponding set of health metrics that contributed to this diagnosis.\n",
    "\n",
    "3. **Mapping from the example data to two destination feature spaces:**\n",
    "   -  Given an audio recording of a bird, map it to two feature spaces: one representing the frequency spectrum of the bird's song and the other representing the temporal pattern of its chirps.\n",
    "\n",
    "4. **Mapping from the two feature spaces above to a decision space:**\n",
    "   -  Map the features extracted from the audio recording (frequency spectrum and temporal pattern) to the decision space of bird species classification (e.g., Robin, Sparrow) based on their unique song characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[1pt]** Explain advantages and disadvantages of Bag Of Words versus Word Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages and disadvantages of Bag of Words (BoW) versus Word Embedding:**\n",
    "\n",
    "**Advantages of Bag of Words (BoW):**\n",
    "- BoW is simple to implement and understand.\n",
    "- It is interpretable because each word is represented by a vector containing word counts.\n",
    "- BoW can be effective for small datasets where complex linguistic structures are less important.\n",
    "\n",
    "**Disadvantages of Bag of Words (BoW):**\n",
    "- It ignores word order, leading to a loss of sequential and contextual information.\n",
    "- BoW can result in high-dimensional vectors for large datasets, leading to computational and storage issues.\n",
    "- BoW vectors are often sparse, containing mostly zeros, which can reduce efficiency in terms of storage and computation.\n",
    "\n",
    "**Advantages of Word Embeddings:**\n",
    "- Word embeddings capture semantic relationships between words, preserving semantic information.\n",
    "- They can reduce the dimensionality of the feature space compared to BoW, improving efficiency and reducing overfitting.\n",
    "- Word embeddings consider the context in which a word appears, capturing contextual information effectively.\n",
    "\n",
    "**Disadvantages of Word Embeddings:**\n",
    "- Word embeddings can be more complex to implement and understand compared to BoW, requiring more computational resources for training and inference.\n",
    "- They may be highly specific to the corpus on which they are trained, limiting generalization to other tasks or domains.\n",
    "- Word embeddings may be less interpretable than BoW, as they represent words as continuous vectors in a vector space rather than discrete word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **[2pt]** Refer to the graph exemplifying Word Embedding in the slides, and its explanation. (i) What does it mean that the point representing Paris is close to the point representing Berlin? (ii) Why is the point for Paris closer to the point for France than to the point representing Italy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(i) When the point representing Paris is close to the point representing Berlin in the Word Embedding graph, it indicates that Paris and Berlin have similar semantic meanings or are used in similar contexts within the corpus from which the Word Embedding model was trained. In other words, the vector representations of Paris and Berlin are close together in the embedding space, suggesting that they share common **attributes** or are often mentioned in similar contexts.\n",
    "\n",
    "(ii) The point for Paris is closer to the point representing France than to the point representing Italy because Word Embedding models capture semantic relationships between words. Since Paris is the capital city of France, it is strongly associated with France in terms of meaning and context. Therefore, in the Word Embedding space, the vector representation of Paris is closer to the vector representation of France than to other countries like Italy, which are not as strongly associated with Paris in terms of semantics and context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Kernels theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[1pt]** Write the definition of kernel function (use latex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel function $( k )$ is a function that computes the inner product of mapped data points in a higher-dimensional feature space induced by a mapping function $( \\phi )$. Mathematically, it is expressed as: \n",
    "$ k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle. $\n",
    "\n",
    "for all $(x, y)$ in the input space $X$. This inner product can also be denoted as $h\\phi(x),\\phi(y)i$ or $k(\\cdot, x), k(\\cdot, y)$, indicating the inner product operation between the mapped points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[2pt]** Explain the kernel trick in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick is a clever technique used in machine learning to handle complex relationships in data. Instead of directly transforming data into a higher-dimensional space, which can be computationally expensive, the kernel trick allows us to compute similarities between data points as if they were in that higher-dimensional space. This trick makes algorithms like Support Vector Machines (SVMs) more efficient and effective in capturing non-linear patterns in data, without the need for explicit feature mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 **[1pt]** Explain in English the required properties of a Mercer kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Mercer kernel is a type of kernel function used in machine learning algorithms, particularly in support vector machines (SVMs). It has two required properties:\n",
    "\n",
    "1. **Symmetry**: A Mercer kernel must be symmetric, meaning that $( k(x_1, x_2) = k(x_2, x_1) )$ for all $( x_1, x_2 )$ in the input space.\n",
    "2. **Positive definiteness**: A Mercer kernel must be positive definite, which means that the Gram matrix $( K )$ with entries $( K_{ij} = k(x_i, x_j) )$ is positive semi-definite for any finite subset $( \\{x_1, \\ldots, x_n\\} )$. This property ensures that the kernel function captures the similarity between data points in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 **[1pt]** Calculate by hand the linear kernel on points $\\{[2,4], [1, -2]\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By hand: \n",
    "\n",
    "To calculate the linear kernel point, we can use the linear kernel formula: \n",
    "$k(x,y) = x^Ty$\n",
    "\n",
    "$x = \\begin{bmatrix} 2 \\\\ 4  \\end{bmatrix}$\n",
    " and \n",
    "$y = \\begin{bmatrix} 1 \\\\ -2  \\end{bmatrix}$ \n",
    "\n",
    "\n",
    "\n",
    "Then we have: \n",
    "$K([2,4],[1,-2]) = \\begin{bmatrix} 2 && 4 \\end{bmatrix}  \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$\n",
    "                 = 2 x 1 + 4 x (-2)\n",
    "                 = 2 - 8\n",
    "                 = -6\n",
    " \n",
    "So, the linear kernel value on points $( [2, 4] )$ and $( [1, -2] )$ is $( -6 )$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 **[1pt]** How do you compute the entry of the Gram matrix for row $i$ and column $j$ for a Gaussian kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the entry of the Gram matrix for row $( i )$ and column $( j )$ using a Gaussian kernel, we use the formula for the Gaussian kernel:\n",
    "\n",
    "$[ k(x, y) = \\exp \\left( -\\gamma \\|x - y\\|^2 \\right) ]$\n",
    "\n",
    "where $( x )$ and $( y )$ are the input data points and $( \\gamma )$ is a hyperparameter.\n",
    "\n",
    "$[ \\text{Gram}_{ij} = \\exp \\left( -\\gamma \\|x_i - x_j\\|^2 \\right) ]$\n",
    "\n",
    "where $( x_i )$ and $( x_j )$ are the $( i )$-th and $( j )$-th data points respectively, and $( \\gamma )$ is the chosen hyperparameter for the Gaussian kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 **[2pt]** Explain why does the Perceptron work with non-linearly separable data using Kernelization. Do you think Linear Regression would work with Kernelization? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron works with non-linearly separable data using Kernelization because it allows the Perceptron to operate in a higher-dimensional space where the data might become linearly separable. Kernelization transforms the input data into a higher-dimensional feature space using a kernel function, where non-linear relationships in the data might become linear. This transformation helps the Perceptron find a separating hyperplane that was not achievable in the original input space.\n",
    "\n",
    "On the other hand, Linear Regression might not work as effectively with Kernelization. Linear Regression aims to find a linear relationship between input features and output labels. Kernelization, by mapping data into a higher-dimensional space, might introduce complexities that Linear Regression cannot effectively handle. Linear Regression assumes a linear relationship between input and output, and if the data is highly non-linear even in the higher-dimensional space, Linear Regression may not capture these relationships accurately. Therefore, while Kernelization can extend the capabilities of some linear algorithms like the Perceptron, it might not be as suitable for Linear Regression due to its inherent linearity assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kernels in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's use once again a two-species adaptation of the Iris dataset. You can copy the code from the last assignment. This time though, to make it harder for linear classifiers let's separate the \"central\" species from the other two. This means that you should set label `versicolor` rather than `setosa` as class `-1`. I suggest you un-comment the `pairplot`s to verify it works.  \n",
    "NOTE: all recommendation on how to handle and prepare the data from the past assignment(s) still hold. As do the warnings that using the wrong data sets will **invalidate the whole answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T08:12:52.600776Z",
     "start_time": "2024-04-21T08:12:52.401319Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "sns.set(rc={'figure.figsize':(8,6)}, style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-21T08:12:52.602299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length    float64\n",
      "sepal_width     float64\n",
      "petal_length    float64\n",
      "petal_width     float64\n",
      "species           int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": "<seaborn.axisgrid.PairGrid at 0x753e6877a130>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset('iris')\n",
    "# sns.pairplot(df, hue='species')\n",
    "\n",
    "df.loc[df['species'] == 'versicolor', 'species'] = -1\n",
    "df.loc[df['species'] != -1, 'species'] = 1\n",
    "df['species'] = pd.to_numeric(df['species'])\n",
    "print(df.dtypes)\n",
    "sns.pairplot(df, hue='species')\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2) # 80-20 split\n",
    "sns.pairplot(test, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 **[2pt]** Train an SVM with linear kernel on the Iris data using Scikit-learn (this time you are required to use class `SVC`). Then do the same using a Gaussian kernel (still `SVC`) and compare the performance using its method `score()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember to prepare inputs/labels for Scikit-learn; again the last assignment should help.\n",
    "- Calling the method `score()` on the trained model just does the prediction and returns the percentage of correct answers. It is a useful function to learn to quickly check if your model is working.\n",
    "- You expect the linear kernel to perform poorly. If the performance is close to the Gaussian kernel, it is possible that the test set was by chance not homogeneous. You can verify that by doing a pairplot on the test set, and if so just run the data loading and preparation again.\n",
    "- No need to find an optimal value for `C` but pass it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "X = df.drop('species', axis=1)\n",
    "y = df['species']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Linear Kernel\n",
    "linear_svm = SVC(kernel='linear', C=1)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "\n",
    "# Gaussian Kernel\n",
    "gamma = 1 / X.shape[1] # default value\n",
    "gaussian_svm = SVC(kernel='rbf', C=1, gamma=gamma)\n",
    "gaussian_svm.fit(X_train, y_train)\n",
    "\n",
    "# Score comparison\n",
    "linear_score = linear_svm.score(X_test, y_test)\n",
    "gaussian_score = gaussian_svm.score(X_test, y_test)\n",
    "\n",
    "print(f\"Linear Kernel SVM Score: {linear_score}\")\n",
    "print(f\"Gaussian Kernel SVM Score: {gaussian_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 **[2pt]** Write a Python function that takes two data points and a value for `gamma` as input, and returns the Gaussian kernel of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian_kernel(x1, x2, gamma):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian kernel between two data points.\n",
    "\n",
    "    Parameters:\n",
    "    x1 : array-like\n",
    "        First data point.\n",
    "    x2 : array-like\n",
    "        Second data point.\n",
    "    gamma : float\n",
    "        Gamma parameter for the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The value of the Gaussian kernel between the two data points.\n",
    "    \"\"\"\n",
    "    # Convert inputs to numpy arrays if they are not already\n",
    "    x1 = np.asarray(x1)\n",
    "    x2 = np.asarray(x2)\n",
    "\n",
    "    # Compute the squared Euclidean distance between the two points\n",
    "    distance = np.linalg.norm(x1 - x2) ** 2\n",
    "\n",
    "    # Compute the Gaussian kernel\n",
    "    kernel_value = np.exp(-gamma * distance)\n",
    "\n",
    "    return kernel_value\n",
    "\n",
    "# Test the function\n",
    "x1 = np.array([2, 4])\n",
    "x2 = np.array([1, -2])\n",
    "gamma = 0.1\n",
    "print(f\"Gaussian Kernel of x and y: {gaussian_kernel(x1, x2, gamma)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 **[3pt]** Write a Python function that takes two dataset (and a gamma) and returns their Gram matrix for a Gaussian kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You need two datasets because you need to compute the *train* matrix between the train and itself, but the *test* matrix between the test and the train.\n",
    "- Simplest method:\n",
    "    - Create a return matrix, initially empty, shaped size_of_A times size_of_B, with dtype 'float64'\n",
    "    - Run two loops with indices (i, j) in ranges up to size_of_A and size_of_B\n",
    "    - Compute the kernel between row i in A and row j in B, and place it in the return matrix at row i column j\n",
    "- Careful with Pandas' `iterrows()`, as the \"index\" it returns is the DataFrame index (i.e. for use with `loc[]`), not the ordinal index (i.e. for `iloc[`). \n",
    "- Generating the matrix automatically is harder, as there is no straightforward way to compute an `outer` in numpy or pandas with a custom function.\n",
    "- One way is to use `column_stack` https://stackoverflow.com/a/21759340 then apply the kernel defined above.\n",
    "- Another is to use `ufunc.outer` http://folk.uio.no/inf3330/scripting/doc/python/NumPy/Numeric/numpy-7.html which is only defined for Universal Functions (`ufuncs`). Look at the examples for `outer`, you can re-implement the function above starting with `np.subtract.outer(A, B)`, which generates the matrix (but check the shape!), then you can run the other operations using broadcast. Both outers and universal functions are super useful, it's worth the effort of learning them, more [[here]](https://docs.scipy.org/doc/numpy/reference/ufuncs.html).\n",
    "- `pandas.apply()` along rows is also an option you should be able to consider with by now. The function name for `-` is `np.subtract` (which is an `ufunc`, see above).\n",
    "\n",
    "Above all, remember the first rule of a good BDD engineer: red, green, refactor! First make it work, then make it better ;) complex solutions are as good as bonus questions here.\n",
    "\n",
    "Also, know that a common default value for gamma is one over the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian_gram_matrix(train_data, test_data, gamma=None):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix for a Gaussian kernel between two datasets.\n",
    "\n",
    "    Parameters:\n",
    "    train_data : array-like\n",
    "        Training dataset.\n",
    "    test_data : array-like\n",
    "        Test dataset.\n",
    "    gamma : float, optional\n",
    "        Gamma parameter for the Gaussian kernel. Default is None, which uses a common default value.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray\n",
    "        The Gram matrix for the Gaussian kernel between the two datasets.\n",
    "    \"\"\"\n",
    "    # If gamma is not provided, set it to a common default value\n",
    "    if gamma is None:\n",
    "        gamma = 1.0 / train_data.shape[1]  # Default value: 1 / number of features\n",
    "\n",
    "    # Initialize an empty Gram matrix\n",
    "    gram_matrix = np.zeros((len(train_data), len(test_data)), dtype=np.float64)\n",
    "\n",
    "    # Compute the Gaussian kernel for each pair of points in the datasets\n",
    "    for i, x1 in enumerate(train_data):\n",
    "        for j, x2 in enumerate(test_data):\n",
    "            gram_matrix[i, j] = np.exp(-gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "\n",
    "    return gram_matrix\n",
    "\n",
    "# Test the function\n",
    "train_data = np.array([[2, 4], [1, -2], [3, 5]])\n",
    "test_data = np.array([[0, 1], [2, 3]])\n",
    "gamma = 0.1\n",
    "gram_matrix = gaussian_gram_matrix(train_data, test_data, gamma)\n",
    "print(\"Gaussian Gram Matrix:\")\n",
    "print(gram_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 **[2pt]** Compute the Gram matrix on the inputs of your datasets. Then train a new SVM, same settings as before with linear kernel, but this time using the Gram matrix('s rows) as the inputs. Print the `score` performance of this new SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With an 80-20 split you are looking at a $120 \\times 120$ shape for the train, and $30 \\times 120$ for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Compute the Gram matrix for the training and test datasets\n",
    "train_gram_matrix = gaussian_gram_matrix(train.values[:, :-1], train.values[:, :-1])\n",
    "test_gram_matrix = gaussian_gram_matrix(test.values[:, :-1], train.values[:, :-1])  # Note: test with respect to train\n",
    "\n",
    "# Train a new SVM with a linear kernel using the Gram matrix's rows as inputs\n",
    "svm_linear_gram = SVC(kernel='linear')\n",
    "svm_linear_gram.fit(train_gram_matrix, train['species'])\n",
    "\n",
    "# Evaluate the performance of the SVM\n",
    "score_linear_gram = svm_linear_gram.score(test_gram_matrix, test['species'])\n",
    "print(\"Linear Kernel SVM with Gram Matrix Score:\", score_linear_gram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 **[1pt]** Plot the confusion matrix for the three SVMs you trained in the past questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's learn a convenient and easy function for this common, very useful metric: `ConfusionMatrixDisplay.from_estimator` [[link here]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator).\n",
    "- So far we saw explicitly 4 cells: true and false positives, true and false negatives. More generally the confusion matrix can be scaled to any number of classes by having the correct labels on the rows, and the predictions on the columns. Errors will be outside the diagonal.\n",
    "- You can use the `normalize` option to get percentages if you like. Which setting do you find most informative?\n",
    "- It's easier if you write a `for` loop over the three models you trained in the previous questions -- just make sure you gave them different names. Also careful as one takes a Gram matrix as input ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Initialize the confusion matrix display\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# List of SVMs and their names\n",
    "svms = [linear_svm, gaussian_svm, svm_linear_gram]\n",
    "svm_names = ['Linear Kernel SVM', 'Gaussian Kernel SVM', 'Linear Kernel SVM with Gram Matrix']\n",
    "\n",
    "# Plot confusion matrices for each SVM\n",
    "for svm, name, ax in zip(svms, svm_names, axes):\n",
    "    if name == 'Linear Kernel SVM with Gram Matrix':\n",
    "        # Predict using the Gram matrix\n",
    "        y_pred = svm.predict(test_gram_matrix)\n",
    "    else:\n",
    "        # Predict using the test data\n",
    "        y_pred = svm.predict(test.drop(columns=['species']))\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(test['species'], y_pred)\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm.classes_)\n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation=45)\n",
    "    disp.ax_.set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the end of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
    "\n",
    "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Use a contour plot to show the classification boundaries of your SVMs. [[link here]](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Learn to search for the best values for `gamma` and `C` [[link here]](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) . NOTE: this is extremely valuable experience for when (not even if!) you will need a SVM for a real application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Generate points to run through with a regression algorithm like Linear Regression from our earlier exercises. This time start from a nonlinear equation (e.g. $x^2$), and add noise as usual. Then try your hand with SVR with a (linear or) nonlinear kernel, which is equivalent to running Linear Regression on the Gram matrix (yet another name: Kernel Ridge Regression) [[link here]](https://scikit-learn.org/stable/modules/kernel_ridge.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I once read a quote that restricting calculus to linear functions is like restricting biology to the study of great apes (help tracking its origin would be welcome). We start from linearity because it's easier to study; the real world is rarely so kind, so learning adaptations such as the kernel trick is simply invaluable.\n",
    "- Trying (scikit-learn) Naïve Bayes or Linear Discriminant Analysis on the Gram matrices would take you just a minute and be invaluable experience. For example, I wouldn't be surprised if LDA performed better than NB (think: why?). But if we had a very large dataset, the Gram matrix would become too large for LDA to handle (remember it does not scale well on the number of features).\n",
    "- **[IMPORTANT]** If you want to gain first-hand experience in tools you can actually use in the real world, consider submitting on the bonus questions from this point on, as I am switching the topic from \"topics for curiosity\" to \"actual deployed value\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
