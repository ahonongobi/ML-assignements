{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You:\n",
    "\n",
    "Teammate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the tenth lab. Neural networks are more a class of tools than a single tool, though the foundation you built last week should enable you to understand what is going on here without too much trouble.\n",
    "\n",
    "There is relatively little coding this week, which is unfortunate: we are starting to touch topics that require more than a lab's worth of practice to achieve basic proficiency. Rather than overloading you of work, this week we focus a bit more on foundations and give you time to study; then we should hit more interesting and fun applications over the next lectures with Deep Learning and Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 14 points (out of 21 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[1pt]** Explain in English what is the distinctive feature of a residual network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[2pt]** Write the full equation of a network with structure [2, 4, 1] (same as last week), but this time add (i) biases on all neurons, and (ii) self-recurrent connections only on the hidden layer. How many weights does this network have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I would suggest starting from your answer from last week, fixing it based on the solution if you need to and have not already, then add what you need.\n",
    "- To avoid changing the indices of the weights, you can simply call bias weights $b$ rather than $w$, and recurrent connections $r$.\n",
    "- The main thing to remember is: each line has the weights entering one destination neuron, and each column refers to one of the inputs to the layer.\n",
    "- Then for a recurrent network, remember to pass the output of all neurons of the same layer (technically representing the previous-step activations, initialized as `0`s) as inputs to each neuron.\n",
    "\n",
    "The network has three layers:\n",
    "- An input layer (no neurons!) with two elements $(x_1, x_2)$\n",
    "- One hidden layer composed of four neurons $(n_1, n_2, n_3, n_4)$\n",
    "- The output layer with only one neuron $(n_5)$\n",
    "\n",
    "We will need to add biases and recurrencies this time: it could be helpful to describe the inputs/outputs for each layer together with the weight matrix. \n",
    "- $X$ is the network input, same as before\n",
    "- Then come the recurrent connections: all the outputs of the neurons of the hidden layer, technically from the previous time step (initialize as zeros)\n",
    "- Finally the bias input, the constant 1 that will be multiplied by the bias weight\n",
    "- $X_{hid}$ is the actual full input to the hidden (and first) layer: all three above\n",
    "- $W_h$ is the weight matrix for ALL the connections entering the hidden layer in the columns, while the rows group the connections entering each neuron\n",
    "- The output can be written with $n_i$ same as we did last time; the don't forget you will need the bias also for the output layer (but no recursion!)\n",
    "- You can call $X_{out}$ the input to the output (and second) layer, and $W_{out}$ its weight matrix.\n",
    "And do not underestimate the value of a quick sketch on a piece of paper! Or head to [draw.io](https://draw.io) if you want a computer drawing that is easy, quick and professional looking.\n",
    "\n",
    "Remember that the output can be interpreted as one scalar, but is in principle a vector with one element (because having only one output is a special case, normally you need a list of outputs here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **[2pt]** A neural network has only one layer of two convolutional neurons with identity activation. Below you will find respective kernels $W_1$ and $W_2$ and input $X$. Activate the network on the input by hand showing all calculation. Assume no padding and state explicitly the expected output size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to understand what you need to explain about your calculations if you actually start doing them :) just mark what you actually input in the calculator, what the calculator returns, and what calculation you are confident skipping.\n",
    "\n",
    "$$\n",
    "W_1 = \n",
    "\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    " 2 &  2 &  2 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix}\n",
    "\\quad,\\quad\n",
    "W_2 = \n",
    "\\begin{pmatrix}\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1 \\\\\n",
    "-1 & 2 & -1\n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "2 & 2 & 2 & 2 & 2 \\\\\n",
    "3 & 3 & 3 & 3 & 3 \\\\\n",
    "1 & 1 & 1 & 1 & 1 \\\\\n",
    "3 & 3 & 3 & 3 & 3 \\\\\n",
    "2 & 2 & 2 & 2 & 2 \n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 **[2pt]** Look at the activations of the two neurons from 1.3 and discuss why they are so different. Explain in particular the regularities both in the inputs and in the kernels. Then go one step further and explain, to the best of your understanding, which types of features are detected by the two kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is another open question: as long as you do not write anything wrong, while showing competence and intuition, you will get the points.\n",
    "- Hint: focus on thinking about the _patterns_ that you can see by eye both in the data matrix and in the kernels. Try to go for an intuitive answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 **[1pt]** Activate a $3x3$ max pooling layer on the outputs of the two convolutions from your answer to question 1.3. Assume no padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 **[1pt]** Explain in one sentence what is an autoencoder. Why do autoencoders have an hourglass shape? Could you design an autoencoder with a different shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[3pt]** Below is last week's implementation of a neural network augmented into a fully-connected RNN with bias connections. Fix it by writing the missing code as marked by `?`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unless otherwise stated, a RNN has fully-connected self-recurrent connections on each layer.\n",
    "- You should know exactly which connections to add if you answered the RNN question in the fundamentals.\n",
    "- For the bias: remember that you need all elements in `state` to be longer by one element: put actual `1`s in these last positions at initialization, then never touch them again.\n",
    "- Recurrencies: you need to make space in the input to each layer for its own output. I typically sort them as [input, recurrencies, bias], but order is not important: consistency is. Make sure all your sizes are correct.\n",
    "- When calculating the size of an input now you need to use `struct` twice: once for the size of the layer entering (something like `struct[nlay]`) and once for the size of the output that goes back as input in the recursion (hence `struct[nlay+1]`). HINT: to make the pairs for each layer execute and understand the following: `zip(struct, struct[1:])`\n",
    "- When you activate a layer remember to copy the activation to both (i) its output and (ii) its input, at the correct indices.\n",
    "- It's easier to compute the size of each input beforehand, then use it to make the `state` list. Then for each weight matrix you can take the number of rows from the structure (as before), and the number of inputs from the `state` sizes.  Don't worry about duplicating the activation in the layer's input, it's actually faster because you have a ready numpy array rather than composing at activation.\n",
    "- Remember to initialize the recurrent output to `0`. Simplest way is to initialize `state` using `np.zeros()` instead of `np.empty()`. Then set the last value of each `state` element to `1` for the bias.\n",
    "- I used myself `import IPython; IPython.embed()` heavily to get this to work. You can also call (once!) `%pdb` to drop in the debugger on error. Keep calm and check the dimensions.\n",
    "- The layer activation function should change because you are saving the activation to two locations (and at specific indices, not direct substitution like before), but the network activation function should change just marginally (add indices to insert input in state)\n",
    "- To _convolve_ with stride 1 and no padding a window of size two on a 1D list (take a pair at a time, advance by one) in Python you can use `zip(lst, lst[1:])`.\n",
    "- This question only refers to neural networks, not learning algorithm, so leave backpropagation out and do not worry about unrolling the network.\n",
    "- Again activate it on a simple input to verify everything is works. The input should be exactly the same as last week's (as the network architecture).\n",
    "- Think: how many weights do you expect to have? Remember that you have 3 matrices (for the 3 layers of neurons), in each the number of rows is unchanged (because you have one per each neuron) but the inputs now are not only connections from the previous layer, you also have recursion (take the output of this layer as its own input) and bias (constant 1 appended to the inputs).\n",
    "<!-- Secret hint: my tally is (4+3+1)*3 + (3+4+1)*4 + (4+3+1)*3 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork:\n",
    "    def __init__(self, struct):\n",
    "        # These are basic, copy+paste from FFNN\n",
    "        self.struct = struct\n",
    "        self.nlayers = ?\n",
    "        ?, ?, ? = self.struct\n",
    "        self.sigma = ?\n",
    "        \n",
    "        # Each `state` is an input for next layer: it now includes rec and bias\n",
    "        state_sizes = [? for inp, rec in zip(self.struct, self.struct[1:])]\n",
    "        # Notice the `zip` above ends when the second list reaches the end (1 shorter)\n",
    "        # Last `state` is only the output of the last layer (no rec/b)\n",
    "        state_sizes += [?]\n",
    "        # We can now build the state of the network\n",
    "        self.state = ?\n",
    "        \n",
    "        # We will need to access inputs and recurrencies by index: the following helps\n",
    "        self.inp_idxs = [range(?, ?) for ? in ?]\n",
    "        self.rec_idxs = [range(?, ?+?) for ?, ? in zip(?, ?[1:])]\n",
    "        # Finally, fix the bias input in the last position of all input `state`s\n",
    "        # Just set and forget, and no need for indices because we won't access it again\n",
    "        for s in self.state: s[-1] = ?\n",
    "            \n",
    "        # The `state` sizes now correspond to the row lengths (ncols) for the weight matrices\n",
    "        self.wsizes = [[?, ?] for ?, ? in zip(?[1:], ?)]\n",
    "        # Finally: weight initialization. Bad practice to hardcode this, but ok here\n",
    "        self.weights = [np.random.normal(size=ws) for ws in self.wsizes]\n",
    "\n",
    "    # The layer activation is unchanged: sigma(W.dot(X)) -- only W and X (=state) differ\n",
    "    def act_layer(self, nlay):\n",
    "        return ?\n",
    "\n",
    "    # The network activation only writes the act twice this time: output & rec-input\n",
    "    def act_net(self, inp):\n",
    "        assert len(inp) == self.nins, f\"got input `{inp}`, expected np.array of length `{self.nins}`\"\n",
    "        self.state[0][self.inp_idxs[0]] = inp\n",
    "        for nlay in range(self.nlayers-1):\n",
    "            act = self.act_layer(nlay)\n",
    "            # This time the layer activation goes in two places:\n",
    "            # - In the input indices of the output of this layer / input to next\n",
    "            self.state[?][self.inp_idxs[?]] = act\n",
    "            # - In the recurrent indices of the input to this layer\n",
    "            self.state[?][self.rec_idxs[?]] = act\n",
    "        return self.state[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = [4,5,4,3]\n",
    "inputs = [3,2,4,3]\n",
    "net = RecurrentNeuralNetwork(struct)\n",
    "# We expect the activation to change this time upon multiple calls on the same input\n",
    "# This is because the `state` of the RNN is maintained in the recurrent connections\n",
    "print(\"activation 1:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 2:\", net.act_net(np.array(inputs)))\n",
    "print(\"activation 3:\", net.act_net(np.array(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[2pt]** Write a Python function for 2D convolution, then run it on a randomly generated input matrix and show the output.\n",
    "\n",
    "- You need to write a function that takes a 2D input and a function to convolve as parameters, then convolves the function over the inputs to produce the output.\n",
    "- The window size is not specified: you can use a 3x3 to keep it simple since you saw that in the examples.\n",
    "- The function to convolve is not specified: no need for it to be a neural network, something as simple as `sum` or a quick lambda calling the numpy `x.sum()` would work perfectly well. Remember that the important part is that it should take a high-dimensional (3x3?) input and output only one value.\n",
    "- The size of the input matrix is not specified: with a 3x3 mask we could go as small as 5x5 with no padding and that would still show that the convolution works.\n",
    "- Then remember: the neural networks are just other functions than `sum`, but behave exactly the same way. Convolving a neural network only allows you to learn the function rather than hardcoding it, but the convolution process is independent.\n",
    "- Answer the question until the end: show that you know how to create a matrix of random numbers, and of the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Handwritten digit recognition with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning, we need to cross a gap in exercise complexity. On one hand you are ready to understand the inner work of a DL library like Keras, on the other asking you for such a task on top of today's lecture is too much even for this course :) So let's leave the creative part for next weeks, where we will see some more advanced applications anyway, and focus today on what we learned and on a new skill: how to justify your code.\n",
    "\n",
    "Below is a tutorial from the Keras website on convolutional networks [[source]](https://keras.io/examples/vision/mnist_convnet/). It uses the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a standard dataset for handwritten character recognition. Keras offers you a backend to automatically download the dataset, similarly to what we did so far with Seaborn and Iris.\n",
    "\n",
    "The code should work as is (did you `pipenv install tensorflow keras`?), but take a while to run. Read it line by line, really study and understand it, feel free to change it so that it runs in few seconds if you wish to play with it; then answer the questions below.\n",
    "\n",
    "NOTE: you don't need to run the code to answer any of the questions below. If you used PyTorch at the last lecture, this is your chance to try out Keras. If you cannot (looking at you M1 users), you can use Colab for this assignment, or replace the Keras tutorial with a PyTorch equivalent [such as this](https://pythonguides.com/pytorch-mnist/). This only affects 4.3, while 4.1 and 4.2 should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Simple MNIST convnet\n",
    "Author: [fchollet](https://twitter.com/fchollet)\n",
    "Date created: 2015/06/19\n",
    "Last modified: 2020/04/21\n",
    "Description: A simple convnet that achieves ~99% test accuracy on MNIST.\n",
    "SOURCE: https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the data\n",
    "\"\"\"\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "## Build the model\n",
    "\"\"\"\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Train the model\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "## Evaluate the trained model\n",
    "\"\"\"\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 **[1pt]** Data plotting: plot the first 9 images in MNIST using a 6x6 subplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see what the MNIST looks like. I showed how to use subplots in a recent solution -- do you remember where it was? You'll need the ability to search quickly for what you need to complete the exam in time. Try timing how long it takes you to answer this question (no seriously challenge your teammate on who solves this the quickest and feel free to brag about it in your solution below).\n",
    "- Add the label on the title to see how the numbers are represented: do you see the connection to last week's `species`?\n",
    "- After you obtain the axis from `plt.subplots()`, you can print a 2D image using `ax[?,?].imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 **[4pt]** Explain the following lines in the Keras MNIST Tutorial code (in English): 27, 48, 51, 52, 57, 67/77, 68/78, 71\n",
    "\n",
    "- To answer this question you need to show complete competence, as if you wrote this code yourself and you were asked to explain your choices at an oral exam.\n",
    "- For each of the lines mentioned, check the code provided and explain it thoroughly\n",
    "- For each variable, explain its meaning, its use, and the choice of value assigned\n",
    "- For each function call, explain what it does, the meaning of all parameters, and the choices of all values.\n",
    "- Reading the code like \"assign 12 to variable `epochs`\" will not constitute an acceptable answer.\n",
    "- Reading the code like \"creates a new Sequential\" is also not acceptable: check the documentation for `Sequential`, understand what the call does, and present your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27, 48, 51, 52, 57, 67/77, 68/78, 71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 **[2pt]** Run Keras MNIST code, tweaking it as needed if it takes too long on your machine. Plot the model's accuracy and loss over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is almost for free since you did the same visualization last week, but you need to get the code to run first.\n",
    "- Also you may want to make sure your changes include setting 'accuracy' and the `history` variable, or at the end of the run you could end up with still nothing to show :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the end of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
    "\n",
    "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Edit the Keras MNIST code to use a simple RNN, then cheat by passing all images of a class in a sequence (careful with batch size). Reset the network between classes. RNNs will recognize that you expect a constant output per each sequence, decide which output with the first few images, then just saturate the right neurons using the recurrent connections to generate a constant output regardless of the input. You can verify this by then testing the network on a sequence of elements from a constant class, followed by one (or more) elements from another class: they will likely be misclassified. All intelligent learning picks up on shortcuts whenever available, here is a famous example (check the full paper): [husky vs. wolf](https://www.researchgate.net/figure/A-husky-on-the-left-is-confused-with-a-wolf-because-the-pixels-on-the-right_fig1_329277474). Notice that getting a \"simple\" RNN in Keras is not straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of this lecture + exercise you should _own_ neural networks. It does not mean that you know everything about them, but you know enough to understand any resource on the topic, and actually understand how these things work better than most people who just use Keras/Pytorch on a daily basis (unfortunately)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
