{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You: **Ahonon Gobi Parfait**\n",
    "\n",
    "Teammate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second lab. I hope this environment is starting to look more familiar, and that you learned some of the shortcuts. At least try to use shortcuts to evaluate a cell, create a cell above or below the current, and switch between code and markdown, as these will dramatically improve your efficiency and significantly cut the time it takes to complete the assignment.  \n",
    "Also, if you have not tried LaTeX in the previous assignment, give it a try in this one $y=mx+q$: a little practice goes a long way for when you will need to write more complex equations and LaTeX will be required (e.g. future assignments + exam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's assignment is likely going to be a bit more time consuming than last week. And there is _a lot_ of Python.  I received multiple emails from people who are not confident in their Python skills or are worried having to learn it now in parallel with the main class content. I understand the concern and will be sure to hold your hand step by step in learning and practicing the required skills even if you have no prior experience.  \n",
    "Please understand nonetheless that proficiency in Python as seen here will be mandatory to pass the exam, so give it your best effort. You may want to start completing this assignment for example a few days before the deadline, to leave yourself time to ask any question on Moodle if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we introduced three libraries: `numpy` does the main number crunching in Python; `matplotlib` is the foundation of most plotting; and `seaborn` wraps the plotting in a convenient interface and eye-pleasing defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we introduce **Scikit-learn**, another heavy-weight library in the field which provides basic (but high-quality and fast) data analysis and ML tools.  \n",
    "Head over to the home page of [Scikit-learn](https://scikit-learn.org/stable/): how many of the concepts are you already familiar with? Over the next week you will become confident in almost each single word used in that page. Also check out the [user manual](https://scikit-learn.org/stable/user_guide.html) for an overview of the methods at your disposal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pts]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 14 points (out of 21 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics of *Classification* and *Feature* are fundamental ML concepts. Today's lecture has barely scratched the surface, especially if you think Data Analysis as a goal rather than a simple step, so we will explore these topics in further detail in the upcoming weeks. Let's make sure to have a solid grasp of these concepts before moving ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's start easy. Here is an example dataset of snakes. It has three fields: `head size`, `length` (in cm) and whether it is `poisonous` or not. It looks like this:\n",
    "\n",
    "```python\n",
    "snakes = [['small', 38, False],\n",
    "          ['small', 62, True],\n",
    "          ['medium', 55, True]]\n",
    "```\n",
    "\n",
    "This simple list of lists puts the emphasis on the data points (the rows), which is an intuitive approach at first for humans to manually write down the data. You will often encounter tables, JSON and CSV files that look like this. We will explore more machine-friendly formats later (spoilers: column-oriented), so understand this approach is just a step for ease of comprehension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lighten the mood by focusing on cats and dogs instead for your exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[1pt]** Name three features that are highly discriminative to classify cats from dogs, and three which are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "R:\n",
    "\n",
    "Highly discriminative features:\n",
    "1. Ear shape\n",
    "2. Whiskers\n",
    "3. Tail length\n",
    "\n",
    "Less discriminative features:\n",
    "1. Coat color\n",
    "2. Paw size\n",
    "3. Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[1pt]** Is \"number of legs\" a numeric feature? Is it discrete or continuous? Is it ordinal? What about \"length of tail\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "\"Number of legs\":\n",
    "- Numeric feature: Yes\n",
    "- Discrete or continuous: Discrete (as it represents a count of legs, which can only take integer values)\n",
    "- Ordinal: No, as there is no inherent order or ranking associated with the number of legs\n",
    "\n",
    "\"Length of tail\":\n",
    "- Numeric feature: Yes\n",
    "- Discrete or continuous: Continuous (as it represents a measurement along a continuous scale)\n",
    "- Ordinal: No, as there is no inherent order or ranking associated with tail length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful not to confuse \"discrete cardinal\" with \"continuous\". If you cannot tell the difference, refresh the concept!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 **[1pt]** Write a dataset named `pets` containing cats and dogs, with at least 3 features and 5 entries. Write the corresponding labels in a variable named `labels`. Save the feature names in a variable named `feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T23:29:17.591221Z",
     "start_time": "2024-03-09T23:29:17.567699Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>size</th>\n",
       "      <th>weight</th>\n",
       "      <th>tail_length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>small</td>\n",
       "      <td>30</td>\n",
       "      <td>short</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>medium</td>\n",
       "      <td>50</td>\n",
       "      <td>medium</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat</td>\n",
       "      <td>medium</td>\n",
       "      <td>35</td>\n",
       "      <td>long</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>large</td>\n",
       "      <td>60</td>\n",
       "      <td>long</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>small</td>\n",
       "      <td>25</td>\n",
       "      <td>short</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species    size  weight tail_length label\n",
       "0     cat   small      30       short   cat\n",
       "1     dog  medium      50      medium   dog\n",
       "2     cat  medium      35        long   cat\n",
       "3     dog   large      60        long   dog\n",
       "4     cat   small      25       short   cat"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "pets = [\n",
    "    ['cat', 'small', 30, 'short'],\n",
    "    ['dog', 'medium', 50, 'medium'],\n",
    "    ['cat', 'medium', 35, 'long'],\n",
    "    ['dog', 'large', 60, 'long'],\n",
    "    ['cat', 'small', 25, 'short']\n",
    "]\n",
    "\n",
    "# Corresponding labels\n",
    "labels = ['cat', 'dog', 'cat', 'dog', 'cat']\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['species', 'size', 'weight', 'tail_length']\n",
    "# data pet features\n",
    "# Pandas to display the dataset (This is just for my own understanding)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(pets, columns=feature_names)\n",
    "df['label'] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to include some small dog and large cat, such that their size is not highly discriminant. You can write more data if you want, but do not go overboard, your goal for now is uniquely to pass the assignment. We will load some demo dataset with Scikit-learn later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human input means human errors: let's validate the length of these lists using `assert`, which is a Python keyword that will raise an error if its parameter is false (and just do nothing otherwise).  \n",
    "Here we use `map` to execute `len` over each element of a list (which here is: for each data row), then check if all values correspond to the length of feature names. The function `all` returns whether all of its arguments have truth value. We can also verify the number of labels against the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:11:49.561338Z",
     "start_time": "2024-03-07T18:11:49.558909Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "assert all(l == len(feature_names) for l in map(len, pets))\n",
    "assert len(pets) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to write a decision tree by hand, that classifies cats from dogs on your dataset, by using a simple chain of `if/else` statements. Do not overlook this task: it is an industry standard to integrate human expert knowledge in an automated ML system.  \n",
    "\n",
    "Include at least two questions, meaning the tree depth (max number of decision nodes between start and leaf) should be at least 2. The leaves should contain decision labels, i.e. either $cat$ or $dog$, though you can have multiple instances of either.  \n",
    "I hope you find it obvious that the labels should not be passed to the function.\n",
    "\n",
    "Do you know the [splat operator](https://codeyarns.github.io/tech/2012-04-25-unpack-operator-in-python.html)? You may find it useful to pass data points to your function. Here is a short demonstration [video](https://www.youtube.com/watch?v=9LrtOJTnwJE).\n",
    "\n",
    "With `map`, `zip` and the splat you should now be able to understand the `transpose()` function from last week:\n",
    "\n",
    "```python\n",
    "transpose = lambda lst: list(map(list, zip(*lst)))\n",
    "```\n",
    "\n",
    "A decent Python skill level is nowadays a mandatory requirement for good Data Analysis or Machine Learning job positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[2pt]** Implement a Decision Tree as a function that takes the features of a data point from the `data` defined above and returns a predicted label using an if/else chain. Run it over your `data` to obtain a list of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T23:29:27.050394Z",
     "start_time": "2024-03-09T23:29:27.045127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'dog', 'dog', 'cat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_label(species, size, weight, tail_length):\n",
    "    if species == 'cat':\n",
    "        if size == 'small':\n",
    "            return 'cat'\n",
    "        elif size == 'medium':\n",
    "            if tail_length == 'short':\n",
    "                return 'cat'\n",
    "            else:\n",
    "                return 'dog'\n",
    "        else:  # size is large\n",
    "            return 'cat'\n",
    "    else:  # species is dog\n",
    "        if weight < 40:\n",
    "            return 'cat'\n",
    "        else:\n",
    "            return 'dog'\n",
    "\n",
    "# Apply the decision tree function to each data point\n",
    "predictions = [predict_label(*datapoint) for datapoint in pets]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly check if it got them all right you can use `zip`, which builds lists taking one element in turn from each of its input lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:18:25.818740Z",
     "start_time": "2024-03-07T18:18:25.815372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 'cat')\n",
      "('dog', 'dog')\n",
      "('dog', 'cat')\n",
      "('dog', 'dog')\n",
      "('cat', 'cat')\n"
     ]
    }
   ],
   "source": [
    "for pair in zip(predictions, labels): print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to properly assess the performance of our classification. This is commonly done using the [**Confusion Matrix**](https://en.wikipedia.org/wiki/Confusion_matrix): two rows and two columns, indicating the count (over the dataset) of\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "% \\hrule\n",
    "\\text{true positives} & \\text{false negatives}\\\\\n",
    "% \\hrule\n",
    "\\text{false positives} & \\text{true negatives}\n",
    "% \\hrule\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "(edit this cell and notice above how you can use multi-line `latex` by wrapping your code in `$$` pairs)   \n",
    "(also it does not matter whether you pick $cat$ or $dog$ as the \"positive\" class, but be careful and consistent)\n",
    "\n",
    "The confusion matrix if the foundation to most loss functions for classification, some of which can be [extremely sophisticated](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient).\n",
    "\n",
    "Perfect classification means no false positives (positive answers for data points that should classify negative) and no false negatives (negative answers for data points that should classify positive) for all data points in your dataset.\n",
    "\n",
    "Hint:\n",
    "```python\n",
    "pos = 'cat'; neg = 'dog'\n",
    "tp = 0; tn = 0; fp = 0; fn = 0\n",
    "for pred, lab in zip(predictions, labels):\n",
    "    # your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[2pt]** Compute and display the Confusion Matrix. If your tree did not achieve perfect classification, write a new version that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:23:32.342060Z",
     "start_time": "2024-03-07T18:23:32.335854Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\t\tPredicted cat\tPredicted dog\n",
      "Actual cat\t\t2\t\t\t1\n",
      "Actual dog\t\t0\t\t\t2\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "pos = 'cat'\n",
    "neg = 'dog'\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Compute confusion matrix\n",
    "for pred, lab in zip(predictions, labels):\n",
    "    if pred == pos and lab == pos:\n",
    "        tp += 1\n",
    "    elif pred == pos and lab == neg:\n",
    "        fp += 1\n",
    "    elif pred == neg and lab == pos:\n",
    "        fn += 1\n",
    "    elif pred == neg and lab == neg:\n",
    "        tn += 1\n",
    "\n",
    "# Display confusion matrix with labels to see clearly which value corresponds to which cell ( just for my own understanding)\n",
    "print(f'Confusion Matrix:')\n",
    "print(f'\\t\\tPredicted {pos}\\tPredicted {neg}')\n",
    "print(f'Actual {pos}\\t\\t{tp}\\t\\t\\t{fn}')\n",
    "print(f'Actual {neg}\\t\\t{fp}\\t\\t\\t{tn}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the result using string interpolation. There are three ways to interpolate strings in Python: using `format()`, using `%` and using f-strings. You can read about [why you should switch to f-strings](https://realpython.com/python-f-strings/), but for now just try using something like this:\n",
    "```python\n",
    "print(f\"tp: {tp}, fn: {fn}\\nfp: {fp}, tn: {tn}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T23:34:34.455510Z",
     "start_time": "2024-03-09T23:34:34.440822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 2, fn: 1\n",
      "fp: 0, tn: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"tp: {tp}, fn: {fn}\\nfp: {fp}, tn: {tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[think: did you just write a decision tree? can you name the features?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The decision tree is written in the function `decision_tree`. The features are `size`, `length` and `poisonous`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing these things by hand can be tedious, but provides a different type of confidence to then go and study the documentation of the library you would rather use in real applications.  \n",
    "Here is the [scikit-learn](https://scikit-learn.org/stable/modules/tree.html) implementation of a decision tree, and here is [the main class](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Let's load the implementation with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:28:55.208713Z",
     "start_time": "2024-03-07T18:28:55.048136Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 **[1pt]** Train a scikit-learn DecisionTreeClassifier on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T18:37:22.253862Z",
     "start_time": "2024-03-07T18:37:22.241493Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'cat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train a scikit-learn DecisionTreeClassifier on the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:242\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    238\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    239\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 242\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:616\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[1;32m    615\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 616\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[1;32m    618\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'cat'"
     ]
    }
   ],
   "source": [
    "# Train a scikit-learn DecisionTreeClassifier on the dataset\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(pets, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 **[1pt]** Compare the two trees (handmade and scikit-learn) in number of leaves, tree depth, selected features, and thresholds (in English)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first proper learning algorithm, and also the first with an iterative implementation. Its implementation is simple: you should use this opportunity to become confident in its features, as we will find them in much more complex algorithms over the next weeks. Any extra work in this section will make the following assignment much, much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 **[1pt]** Write the equation of an hyperplane in $\\mathbb{R}^k$, specifying the numeric set and dimensionality of each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "R: The equation of an hyperplane in $\\mathbb{R}^k$ is given by:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k = 0 $\n",
    "\n",
    "where $\\beta_0, \\beta_1, \\ldots, \\beta_k$ are the parameters of the hyperplane and $x_1, x_2, \\ldots, x_k$ are the features of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 **[1pt]** Write the definition of *Linearly Separable Dataset* in plain English (no math)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "R: A dataset is **linearly separable** if it is possible to draw a straight line (or a hyperplane in higher dimensions) that separates the data points of different classes. In other words, there exists a clear boundary that can separate the data points of one class from those of another class without any overlap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next question, let's make sure the concept of Margin and its use is clear. Here is an example of point $(x, y)$ and hyperplane parametrization (w, b).\n",
    "\n",
    "$$\n",
    "\\text{point: }\\;\\; ((1, 3, -5, -2), +1)\\\\\n",
    "\\text{params: }\\;\\; ((2, 7, -3, 5), -2)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 **[1pt]**  Compute by hand (LaTeX not Python!) the *margin* for the point and hyperplane above: is the point correctly classified by the hyperplane? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "point: $((1, 3, -5, -2), +1)$\n",
    "params : $((2, 7, -3, 5), -2)$\n",
    "\n",
    "We can calculate the margin using the formula:   \n",
    "margin = $ y \\cdot \\frac{\\langle w, x \\rangle + b}{\\|w\\|} $  \n",
    "where $y$ is the label of the point, $\\langle w, x \\rangle$ is the inner product of the weight vector and the input vector, $b$ is the bias and $\\|w\\|$ is the norm of the weight vector.\n",
    "\n",
    "margin = $ 1 \\cdot \\frac{\\langle (2, 7, -3, 5), (1, 3, -5, -2) \\rangle - 2}{\\|(2, 7, -3, 5)\\|} $\n",
    "\n",
    "margin = $ 1 \\cdot \\frac{(2 \\cdot 1) + (7 \\cdot 3) + (-3 \\cdot -5) + (5 \\cdot -2) - 2}{\\sqrt{2^2 + 7^2 + (-3)^2 + 5^2}} $\n",
    "\n",
    "margin = $ 1 \\cdot \\frac{2 + 21 + 15 - 10 - 2}{\\sqrt{4 + 49 + 9 + 25}} $\n",
    "\n",
    "margin = $ 1 \\cdot \\frac{26}{\\sqrt{87}} $\n",
    "\n",
    "margin = $ 1 \\cdot \\frac{26}{9.327$\n",
    "\n",
    "margin = $ 2.79 $\n",
    "\n",
    "The margin is positive, which means that the point is correctly classified by the hyperplane. This is because the point lies on the correct side of the hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next question we use `numpy`.  \n",
    "We use the definition of Affine Function for the parametrization: here `point` has been conveniently augmented with a trailing $1$ representing the constant input for bias.  \n",
    "Careful from now on you will need to take care of that yourself, starting from one of the next questions. Here are two examples (marked `1` and `2`) of how it can be done.\n",
    "\n",
    "```python\n",
    "x = [1, 3, -5, -2]\n",
    "y = 1\n",
    "w = [2, 7, -3, 5]\n",
    "b = -2\n",
    "point = [np.array([*x, 1]), y] # 1\n",
    "params = np.append(np.array(w), b) # 2\n",
    "```\n",
    "\n",
    "Use `np.dot()` to compute the inner product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T19:33:47.168729Z",
     "start_time": "2024-03-07T19:33:47.164828Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = [1, 3, -5, -2]\n",
    "y = 1\n",
    "w = [2, 7, -3, 5]\n",
    "b = -2\n",
    "point = [np.array([*x, 1]), y] # Augmenting the point with a trailing 1 for bias\n",
    "params = np.append(np.array(w), b)\n",
    "\n",
    "print(point)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 **[1pt]**  Write a function that takes in input an hyperplane parametrization and a point, computes the margin, and returns a boolean indicating whether the classification is correct or not. Run it on the point and parametrization provided below (\"The Inputs\"), and print whether the classification is correct or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T16:59:33.565557Z",
     "start_time": "2024-03-08T16:59:33.557828Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The Inputs -- do not change\n",
    "x = [1, 3, -5, -2]\n",
    "y = 1\n",
    "w = [2, 7, -3, 5]\n",
    "b = -2\n",
    "point = [np.array([*x, 1]), y] # Augmenting the point with a trailing 1 for bias\n",
    "params = np.append(np.array(w), b) # Concatenating weights and bias\n",
    "\n",
    "def compute_margin_and_classification(params, point):\n",
    "    # Extract features and label from the point\n",
    "    x = point[0][:-1]  # Extracting features, excluding the trailing 1 for bias\n",
    "    y = point[1]       # Extracting label\n",
    "\n",
    "    # Compute the inner product of weights and features, and add the bias term\n",
    "    margin_val = np.dot(params[:-1], x) + params[-1]  # Compute margin\n",
    "\n",
    "    # Check if the classification is correct\n",
    "    if y * margin_val > 0:  # If the sign is positive, it's correctly classified\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Compute margin and classification\n",
    "correct_classification = compute_margin_and_classification(params, point)\n",
    "\n",
    "# Print the result\n",
    "if correct_classification:\n",
    "    print(\"The classification is correct.\")\n",
    "else:\n",
    "    print(\"The classification is incorrect.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 **[1pt]** Implement the Perceptron update rule for a single point as a method that takes a hyperplane parametrization and a point (which we assume we know is misclassified) and returns the updated parametrization. Run it on The Input above and print the updated parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:43:10.420877Z",
     "start_time": "2024-03-08T17:43:10.416551Z"
    }
   },
   "outputs": [],
   "source": [
    "def perceptron_update(w, b, x, y):\n",
    "    \"\"\"\n",
    "    Perceptron update rule for a single misclassified point.\n",
    "    \n",
    "    Parameters:\n",
    "        w (list): Weights vector.\n",
    "        b (float): Bias term.\n",
    "        x (list): Features of the misclassified point.\n",
    "        y (int): True label of the misclassified point (+1 or -1).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Updated weights vector and bias term.\n",
    "    \"\"\"\n",
    "    # Update weights\n",
    "    for i in range(len(w)):\n",
    "        w[i] += y * x[i]\n",
    "\n",
    "    # Update bias\n",
    "    b += y\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# The Inputs -- do not change\n",
    "x = [1, 3, -5, -2]\n",
    "y = 1\n",
    "w = [2, 7, -3, 5]\n",
    "b = -2\n",
    "\n",
    "# Perform perceptron update\n",
    "updated_w, updated_b = perceptron_update(w, b, x, y)\n",
    "\n",
    "# Print the updated parametrization\n",
    "print(\"Updated weights:\", updated_w)\n",
    "print(\"Updated bias:\", updated_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 **[1pt]** Print whether the updated parametrization from the last question correctly classifies the point from The Input (use the margin-based function you wrote to answer two questions above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, do you feel confident of your implementation so far? Let's scale it up: implement the Perceptron Algorithm and run it on a demo dataset from Scikit-learn.\n",
    "\n",
    "First we load the classic [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html#sphx-glr-auto-examples-datasets-plot-iris-dataset-py), its history is very interesting so make sure to [have a look at it](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
    "Since we are studying linear binary classification, let's collapse two classes together and focus on two if its four features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:15:04.895704Z",
     "start_time": "2024-03-08T17:15:04.891196Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_x, iris_y = load_iris(return_X_y=True) # print these to understand\n",
    "x1 = np.array([r[0] for r in iris_x]) # first feature\n",
    "x2 = np.array([r[2] for r in iris_x]) # third feature\n",
    "x = np.array([x1, x2]).transpose() # numpy transpose() for free\n",
    "# Reduce to two binary classes {+1, -1}\n",
    "labels = np.array([-1 if y==0 else +1 for y in iris_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Perceptron inputs should be $x$ input vector and $labels$ target labels / classes.  \n",
    "We learned last week what we need to plot such a dataset, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:15:36.817575Z",
     "start_time": "2024-03-08T17:15:36.811834Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(8,6)}, style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:15:45.423433Z",
     "start_time": "2024-03-08T17:15:45.225078Z"
    }
   },
   "outputs": [],
   "source": [
    "def todays_plot(): # learn to write your own\n",
    "    sns.scatterplot(x=x1, y=x2,\n",
    "      hue=labels, # let's use different colors for the two classes\n",
    "      palette=sns.color_palette(['darkred', 'darkblue']),\n",
    "      s=100)\n",
    "\n",
    "todays_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is clearly linearly separable. Let's see how the Perceptron performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 **[4pt]** Implement the Perceptron Algorithm in a single cell (do not call any function defined above) as a function that takes the input vector and target labels and returns a trained hyperplane parametrization. Run it on the (partial) Iris data loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:20:48.761638Z",
     "start_time": "2024-03-08T17:20:48.072505Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron_algorithm(x, labels, learning_rate=1, epochs=1000):\n",
    "    # Initialize weights and bias to zeros\n",
    "    weights = np.zeros(x.shape[1])\n",
    "    bias = 0\n",
    "\n",
    "    # Iterate through epochs\n",
    "    for _ in range(epochs):\n",
    "        # Iterate through each sample\n",
    "        for i in range(len(x)):\n",
    "            # Compute the predicted label\n",
    "            prediction = np.dot(weights, x[i]) + bias\n",
    "\n",
    "            # Update weights and bias if prediction is incorrect\n",
    "            if labels[i] * prediction <= 0:\n",
    "                weights += learning_rate * labels[i] * x[i]\n",
    "                bias += learning_rate * labels[i]\n",
    "\n",
    "    # Return the trained hyperplane parametrization\n",
    "    return np.append(weights, bias)\n",
    "\n",
    "# Example usage on Iris data\n",
    "hyperplane_params = perceptron_algorithm(x, labels)\n",
    "\n",
    "# Print the trained hyperplane parametrization\n",
    "print(\"Trained hyperplane parametrization:\", hyperplane_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we can plot the points and we can plot a $y = mx + q$ model. But how can we plot the $f(x) = \\langle w, x \\rangle + b$ decision boundary? Well we know the two classes are $f(x)>0$ (positive, above) and $f(x)<0$ (negative, below), so our boundary is in $f(x) = 0$. Then we can find the function coefficients in the $w_1, w_2$ space as:\n",
    "\n",
    "$$\n",
    "f(x) = \\langle w, x \\rangle + b = 0\\\\\n",
    "w = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix}\\\\\n",
    "ax + by + c = 0\\\\\n",
    "y = mx + q\\\\[2ex]\n",
    "a = w_1,\\;\\; b=w_2,\\;\\; c = b\\\\\n",
    "w_1 x + w_2 y + b = 0\\\\\n",
    "-w_2 y = w_1 x + b\\\\\n",
    "y = \\frac{w_1}{-w_2} x + \\frac{b}{-w_2}\\\\[2ex]\n",
    "m = \\frac{w_1}{-w_2},\\;\\; q = \\frac{b}{-w_2}\\\\\n",
    "y = \\frac{w_1}{-w_2} x + \\frac{b}{-w_2}\n",
    "$$\n",
    "\n",
    "For now you can use the implementation below, but make sure you understand these simple (if tedious) steps above because next time you will need to implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:29:22.528900Z",
     "start_time": "2024-03-08T17:29:22.525639Z"
    }
   },
   "outputs": [],
   "source": [
    "def wb2mq(w, b):\n",
    "    assert len(w) == 2, \"This implementation only works in 2D\"\n",
    "    assert w[0] != 0 and w[1] != 0 and b != 0 # avoid edge cases for now\n",
    "    return [w[0]/-w[1], b/-w[1]] # m and q\n",
    "\n",
    "def params2boundary(w, b):\n",
    "    m, q = wb2mq(w, b)\n",
    "    print(f\"m: {m}, q: {q}\")\n",
    "    return lambda x: m*x + q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope the code above comes to no surprise after having derived it mathematically. If you read it aloud it should sound obvious in English; if not go back and make sure you understand the mathematical derivation (and the code above) before moving forward or it is only going to be more confusing.  \n",
    "Now we can plot the model on top of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:29:53.574702Z",
     "start_time": "2024-03-08T17:29:53.340775Z"
    }
   },
   "outputs": [],
   "source": [
    "todays_plot()\n",
    "perc_w, perc_b = perceptron(x, labels)\n",
    "# we can use the splat to separate the bias (last element)\n",
    "print(f\"w: {perc_w}, b: {perc_b}\")\n",
    "perc_boundary = params2boundary(perc_w, perc_b)\n",
    "sns.lineplot(x=x1, y=[perc_boundary(inp) for inp in x1], color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Let's do the same with the scikit-learn Perceptron and we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 **[1pt]** Train a scikit-learn Perceptron on the same data as the last question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it should take *no more than two lines*. You should still have the documentation open, the class you are looking for is `Perceptron`, and the method you need is typically called `fit` for most sklearn algorithms. Find out how it works and how to pass features and labels as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:34:54.476070Z",
     "start_time": "2024-03-08T17:34:54.471938Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron_model = Perceptron().fit(x, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the model boundary we need extract the vectors $w$ and $b$ from a trained scikit-learn Perceptron: they are stored as `coef_` (coefficients stands for weights) and `intercept_` (which is another term for `q` or bias) _(as mentioned: be flexible with the nomenclature as each library adopts its own)_.  \n",
    "Note though that they are both `list`s, because the algorithm is written to scale to _multiclass classification_, where you have multiple classes and therefore need multiple hyperplanes to partition the space in multiple regions. Of course with two classes you only need one hyperplane so be sure to access only the first element.  \n",
    "I will be providing the code below yet again, but make sure you study and understand it so you will be able to write it yourself in the next assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T17:36:40.054208Z",
     "start_time": "2024-03-08T17:36:39.279723Z"
    }
   },
   "outputs": [],
   "source": [
    "trained = perceptron_model\n",
    "\n",
    "w, b = [trained.coef_[0], trained.intercept_[0]]\n",
    "d_boundary = params2boundary(w, b)\n",
    "todays_plot()\n",
    "boundary_preds = [d_boundary(inp) for inp in x1]\n",
    "sns.lineplot(x=x1, y=boundary_preds, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 **[1pt]** Compare the resulting boundary against the one trained with your hand-made algorithm, and hypothesize why _in your opinion_ they are similar/different (in English)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The resulting boundary trained with the hand-made algorithm and the one trained with the scikit-learn Perceptron are similar. This is because both the hand-made algorithm and the scikit-learn Perceptron use the same underlying algorithm to train the model. The Perceptron algorithm is a simple linear classification algorithm that learns the weights and bias by updating them iteratively until the model correctly classifies all the training samples. Therefore, the resulting boundary trained with the hand-made algorithm and the one trained with the scikit-learn Perceptron are similar because they are both trained using the same underlying algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise (as many more in the future) trains your ability to express your opinion and  fundamental understanding of the topic using technical language. It is scored based on your expressiveness, not on the correctness (although you should be able to formulate a correct opinion here) or on the English per se. Show your reasoning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
