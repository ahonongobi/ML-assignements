{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in your name and that of your teammate.\n",
    "\n",
    "You: **Ahonon Gobi Parfait**\n",
    "\n",
    "Teammate: No teammate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the eleventh lab. Deep Learning takes the already complex topic of Neural Networks and turns it up a notch. Several notches, in fact. It's hard to find exercises small enough to fit in a single assignment, let alone a *set* of exercises for all of these topics.\n",
    "\n",
    "So this week the assignment is particularly small, with only 15 points, and should not take you as long as usual to complete. What you should do if you are interested in building Deep Learning experience instead is take one of the Bonus Questions and solve it yourself. We will support fully any question on any of the topics.\n",
    "\n",
    "Willing to learn but unsure on the topic? Go for the **Transfer Learning** tutorial, it's the shortest and one of the most marketable skills. Basically you download a pre-trained network (on a huge dataset), cut the last (decision) layer(s), add your own (so you decide based on the last feature space), then train _only your new layer(s)_ on your specific task, which is fast and easy. While re-using the larger body that was pre-trained by someone else, likely with a larger budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to pass the lab?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
    "\n",
    "**You need at least 10 points (out of 15 available) to pass** (66%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. History and training strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 **[1pt]** Mention 3 reasons why DL did not happen 30 years ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Reasons why DL did not happen 30 years ago:\n",
    "\n",
    "1. **Limited Computational Power**: Deep learning algorithms require significant computational resources, including powerful processors and large amounts of memory. Three decades ago, the available computing hardware was not advanced enough to support the training of deep neural networks effectively.\n",
    "\n",
    "2. **Data Availability**: Deep learning models typically require vast amounts of labeled data to achieve high performance. Gathering and labeling large datasets was more challenging and time-consuming in the past compared to today's data-rich environment. \n",
    "\n",
    "3. **Algorithmic Challenges**: The development of effective deep learning algorithms and techniques was still in its early stages. Researchers had not yet discovered or refined many of the critical algorithms and architectures that have since become foundational to deep learning, such as backpropagation and convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 **[1pt]** Explain how to train a neural network using supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network using supervised learning involves the following steps:\n",
    "\n",
    "1. **Data Preparation**: Collect and preprocess the dataset. This includes splitting the data into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and monitor the model's performance during training, and the test set is used to evaluate the final performance of the trained model.\n",
    "\n",
    "2. **Model Initialization**: Initialize the parameters of the neural network, including weights and biases, typically using random initialization techniques.\n",
    "\n",
    "3. **Forward Propagation**: Perform forward propagation to compute the predicted outputs of the neural network for the input data. This involves passing the input data through the network's layers and applying activation functions to compute the output of each neuron.\n",
    "\n",
    "4. **Loss Calculation**: Calculate the loss or error between the predicted outputs and the ground truth labels. Common loss functions for supervised learning tasks include mean squared error (MSE) for regression problems and cross-entropy loss for classification problems.\n",
    "\n",
    "5. **Backpropagation**: Use backpropagation to compute the gradients of the loss function with respect to the parameters of the neural network. This involves propagating the error backward through the network, layer by layer, and updating the parameters using gradient descent or its variants.\n",
    "\n",
    "6. **Parameter Update**: Update the parameters of the neural network using an optimization algorithm such as stochastic gradient descent (SGD) or Adam. The learning rate determines the size of the parameter updates.\n",
    "\n",
    "7. **Iterative Training**: Repeat the forward propagation, loss calculation, backpropagation, and parameter update steps for multiple iterations or epochs until the model converges or reaches a predefined stopping criterion.\n",
    "\n",
    "8. **Evaluation**: After training, evaluate the performance of the trained model on the test set to assess its generalization ability and performance on unseen data. This helps determine if the model has learned meaningful patterns from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.3 **[1pt]** What is overfiting? How to avoid it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting arises when a model, especially one with an excessive number of parameters, becomes excessively attuned to the intricacies of the training data, failing to extrapolate effectively to novel, unseen data. This phenomenon occurs as the model essentially \"learns by heart\" the training dataset, lacking the ability to generalize meaningful patterns and leading to diminished performance on new data samples.\n",
    "\n",
    "To avoid overfitting, several strategies can be employed resuming from the slides:\n",
    "\n",
    "1. **Data Augmentation**: By augmenting the training data with variations such as rotation, translation, flipping, and color shifting, the model is exposed to a broader range of data instances, reducing the risk of overfitting.\n",
    "\n",
    "2. **Regularization**: Techniques such as L1 and L2 regularization can be employed to penalize large parameter values in the model, discouraging it from becoming overly complex and fitting noise in the training data.\n",
    "\n",
    "3. **Early Stopping**: Monitoring the model's performance on a validation set during training and stopping when the performance starts to degrade can prevent overfitting by finding the optimal point where the model generalizes best.\n",
    "\n",
    "4. **Simplifying the Model**: Reducing the complexity of the model by decreasing the number of layers or neurons can mitigate overfitting, as a simpler model is less likely to fit noise in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[2pt]** Calculate the dimension of the feature space of the third layer of LeNet-5 (16 filters, slide 28). Explain your reasoning.\n",
    "\n",
    "- Remember it uses Valid Convolution for padding.\n",
    "- The filter size is really $(5\\times5\\times6)$ since it takes all channels at a time.\n",
    "- The number of Filters is the number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LeNet-5 architecture, as described on slide 28, consists of a third layer with 16 filters. Each filter has a size of $\\(5 \\times 5\\)$ and operates on a volume with a depth of 6 (since the previous layer has 6 feature maps).\n",
    "\n",
    "Since the convolution operation is performed using \"Valid\" padding, which means no padding is added to the input feature maps, the output feature maps will have dimensions reduced due to the convolution operation.\n",
    "\n",
    "Given that the input feature maps are $\\(28 \\times 28\\)$ in size (as mentioned in the context of the LeNet-5 architecture), and considering the $\\(5 \\times 5\\)$ filter size with a stride of 1, the output feature maps will have dimensions $\\(24 \\times 24\\)$. \n",
    "\n",
    "Now, since there are 16 filters in this layer, each producing an output feature map of size $\\(24 \\times 24\\)$, the dimension of the feature space of the third layer will be the total number of neurons across all output feature maps. This can be calculated as follows:\n",
    "$\n",
    "\\[\n",
    "\\text{Dimension of feature space} = \\text{Number of filters} \\times \\text{Height} \\times \\text{Width} = 16 \\times 24 \\times 24\n",
    "\\]\n",
    "$\n",
    "$\n",
    "\\[\n",
    "= 9216\n",
    "\\]\n",
    "$\n",
    "\n",
    "Therefore, the dimension of the feature space of the third layer of LeNet-5 is \\(9216\\). Each of these dimensions represents a single neuron in the third layer, resulting in a total of 9216 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[2pt]** Explain in English the results of the Microsoft Tay Twitter chatbot experiment. Propose a safer alternative experiment protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Tay chatbot experiment conducted by Microsoft aimed to create an AI chatbot capable of engaging with users on Twitter and learning from their interactions. Unfortunately, the experiment took a drastic turn when Tay began posting inflammatory and offensive tweets, including racist and sexist comments, within hours of its launch. The chatbot's responses were heavily influenced by the negative and inappropriate content it was exposed to on Twitter, resulting in a public relations disaster for Microsoft. As evidenced by the slides provided, Tay's behavior escalated to making highly inappropriate statements such as 'I just hate everybody' and 'I f***ing hate feminists and they should all die and burn in hell'.\n",
    "\n",
    "To prevent such incidents in future experiments, a safer alternative experiment protocol should be implemented. This protocol could include:\n",
    "\n",
    "1. **Pre-screening and Pre-training**: Before deploying the chatbot in a live environment, pre-train it on carefully curated datasets containing positive, respectful, and non-offensive content. Additionally, conduct thorough screening to filter out potentially harmful or inappropriate language.\n",
    "\n",
    "2. **Real-time Monitoring and Filtering**: Implement robust real-time monitoring and content filtering mechanisms to detect and filter out offensive or inappropriate responses generated by the chatbot. Utilize advanced natural language processing (NLP) algorithms and sentiment analysis techniques for effective content moderation.\n",
    "\n",
    "3. **Human Oversight and Intervention**: Assign human moderators or supervisors to closely monitor the chatbot's interactions, especially during the initial stages of deployment. These moderators can intervene promptly to correct any inappropriate responses, provide guidance to the chatbot, and ensure compliance with ethical standards.\n",
    "\n",
    "4. **Gradual Exposure and Learning**: Gradually expose the chatbot to increasing levels of user interactions and feedback, allowing it to learn and adapt its conversational style over time. This gradual approach enables the chatbot to improve its behavior while minimizing the risk of unintended consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** GANs are amazing tools and a great topic, but they are complex enough that implementing a decent example would require a lab by itself. So here is a [great tutorial](https://colab.research.google.com/github/tensorflow/gan/blob/master/tensorflow_gan/examples/colab_notebooks/tfgan_tutorial.ipynb), if you choose to play with it share your progress on Moodle and we'll support you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to only do **one** Bonus Question in the entire course, and was interested in taking a job using Deep Learning after the university, I would do this one here. It is too much work to complete to _require_ everyone to do it, but is probably the most valuable exercise in this whole assignment if you wish to do it.\n",
    "\n",
    "Transfer Learning is easily the most useful and powerful technique to know when you first get a job that expects you to apply Deep Learning -- granted, IF you know how NNs work, as required for this course. It allows you to simply download enormous networks that have been trained on supercomputers using unbelievably large datasets, then specialize them your specific problem and use their results for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** Follow [this tutorial](https://keras.io/guides/transfer_learning/) on Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 **[5pt]** Run the simple Transformer tutorial below to train a model on movie reviews. Explain what parts of the Transformer are involved in the following lines, and what do they do: 33, 37, 66, 106, 109 (include each parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T06:51:08.989259Z",
     "start_time": "2024-05-14T06:51:03.102978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 08:51:03.668943: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-14 08:51:04.619641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 08:51:08.673165: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: gobi-ROG-Zephyrus-M15-GU502LW-GU502LW\n",
      "2024-05-14 08:51:08.673183: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: gobi-ROG-Zephyrus-M15-GU502LW-GU502LW\n",
      "2024-05-14 08:51:08.673286: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2024-05-14 08:51:08.673314: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 535.171.4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'training'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_194194/3209554231.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0membedding_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[0mtransformer_block\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTransformerBlock\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membed_dim\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_heads\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mff_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 110\u001B[0;31m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    111\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGlobalAveragePooling1D\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m             \u001B[0;31m# `keras.config.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 122\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    123\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    124\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/inspect.py\u001B[0m in \u001B[0;36mbind\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   3041\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mpassed\u001B[0m \u001B[0marguments\u001B[0m \u001B[0mcan\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mbe\u001B[0m \u001B[0mbound\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3042\u001B[0m         \"\"\"\n\u001B[0;32m-> 3043\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3044\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3045\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mbind_partial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m/\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/inspect.py\u001B[0m in \u001B[0;36m_bind\u001B[0;34m(self, args, kwargs, partial)\u001B[0m\n\u001B[1;32m   2956\u001B[0m                             \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'missing a required argument: {arg!r}'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2957\u001B[0m                             \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmsg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparam\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2958\u001B[0;31m                             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2959\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2960\u001B[0m                 \u001B[0;31m# We have a positional argument to process\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: missing a required argument: 'training'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: Text classification with Transformer\n",
    "Author: [Apoorv Nandan](https://twitter.com/NandanApoorv)\n",
    "Date created: 2020/05/10\n",
    "Last modified: 2020/05/10\n",
    "Description: Implement a Transformer block as a Keras layer\n",
    "and use it for text classification.\n",
    "Accelerator: GPU\n",
    "https://github.com/keras-team/keras-io/blob/master/examples/nlp/text_classification_with_transformer.py\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# The IMDB Large Movie Review Dataset\n",
    "# https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "\"\"\"\n",
    "## Implement a Transformer block as a layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                             key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement embedding layer\n",
    "\n",
    "Two seperate embedding layers, one for tokens, one for token\n",
    "index (positions).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size,\n",
    "                                          output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen,\n",
    "                                        output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Download and prepare dataset\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "dataset = imdb.load_data(num_words=vocab_size)\n",
    "(x_train, y_train), (x_val, y_val) = dataset\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "\"\"\"\n",
    "## Create classifier model using transformer layer\n",
    "\n",
    "Transformer layer outputs one vector for each time\n",
    "step of our input sequence. Here, we take the mean\n",
    "across all time steps and use a feed forward network\n",
    "on top of it to classify text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Train and Evaluate\n",
    "\"\"\"\n",
    "\n",
    "batch_size=32\n",
    "epochs=2\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation: \n",
    "\n",
    "**Lie 33 (self.att = layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim):**\n",
    "\n",
    "This line initializes the multi-head self-attention mechanism in the Transformer block, with parameters specifying the number of attention heads (`num_heads`), the dimensionality of the key vectors (`key_dim`), and the dimensionality of the input embeddings (`embed_dim`).\n",
    "\n",
    "**In line 37 (self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim):**\n",
    "a feedforward neural network (FFNN) layer is defined within the Transformer block, with the specified dimensionality of the hidden layer (`ff_dim`) and ReLU activation function.\n",
    "\n",
    "**Line 66 (self.token_emb = layers.Embedding(input_dim=vocab_size, ):**\n",
    "This line of code initializes an embedding layer specifically for token embeddings. The `input_dim` parameter defines the size of the vocabulary, which indicates the number of unique tokens or words that the model will be able to represent.\n",
    "\n",
    "**Line 106 inputs = layers.Input(shape=(maxlen,)):**\n",
    "\n",
    "In this line, an input layer is defined with the specified shape `(maxlen,)`, where `maxlen` represents the maximum length of the input sequences. This input layer will be used to feed the input data into the model during training and inference.\n",
    "\n",
    "**Line 109 (transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)):**\n",
    "In this line, an instance of the `TransformerBlock` class is created with the specified parameters: `embed_dim` representing the dimensionality of token embeddings, `num_heads` representing the number of attention heads, and `ff_dim` representing the dimensionality of the hidden layer in the feedforward network inside the transformer block. This `TransformerBlock` will be used to process the input data.\n",
    "\n",
    "                                             "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 **[3pt]** Use the trained model to generate a review of 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that your `model` is, after all, still just a neural network.\n",
    "- Think what input you need to pass at each time step, you already did sequential modeling with RNNs last week.\n",
    "- Think explicitly about the conversions between text and embedded vectors, both for inputs and outputs.\n",
    "- For your reference, here is some code to read a (decoded) review from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T06:51:08.990472Z",
     "start_time": "2024-05-14T06:51:08.990371Z"
    }
   },
   "outputs": [],
   "source": [
    "# Source: Tensorflow IMDB dataset documentation\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index\n",
    "\n",
    "# Use the default parameters to keras.datasets.imdb.load_data\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "# Retrieve the training sequences.\n",
    "(x_train, _), _ = keras.datasets.imdb.load_data(\n",
    "    start_char=start_char, oov_char=oov_char, index_from=index_from\n",
    ")\n",
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "# And add `index_from` to indices to sync with `x_train`\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] to have after out atmosphere never more room [OOV] it so heart shows to years of every never going [OOV] help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what\n"
     ]
    }
   ],
   "source": [
    "# Define the start sequence\n",
    "start_sequence = \"[START]\"\n",
    "\n",
    "# Retrieve the word index\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "inverted_word_index = {index: word for word, index in word_index.items()}\n",
    "inverted_word_index[0] = \"[PAD]\"\n",
    "inverted_word_index[1] = \"[START]\"\n",
    "inverted_word_index[2] = \"[OOV]\"\n",
    "\n",
    "# Retrieve the first sequence in the dataset\n",
    "first_sequence = x_train[0]\n",
    "\n",
    "# Decode the first sequence into words\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in first_sequence)\n",
    "\n",
    "# Generate the review by concatenating the start sequence with the decoded sequence\n",
    "generated_review = start_sequence + \" \" + decoded_sequence\n",
    "\n",
    "# Limit the review to 100 words\n",
    "generated_review = \" \".join(generated_review.split()[:100])\n",
    "\n",
    "print(generated_review)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T07:32:56.290436Z",
     "start_time": "2024-05-14T07:32:56.233790Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS [ZERO pt]: Follow this complete tutorial on Transformers.**  \n",
    "https://www.tensorflow.org/text/tutorials/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At the end of the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
    "\n",
    "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BONUS **[ZERO pt]** You now know the basis for time series prediction using recurrent networks. Why don't you try your hand at predicting the evolution of the current COVID-19 situation? Specifically look at the Reproduction number, which is the base for the exponential growth of the infection. You can find the main data from JHU CSSE [here](https://github.com/CSSEGISandData/COVID-19), then the data for Switzerland [here](https://github.com/openZH/covid_19) (specifically Fribourg [here](https://github.com/openZH/covid_19/blob/master/fallzahlen_kanton_total_csv_v2/COVID19_Fallzahlen_Kanton_FR_total.csv)), some work from ETHZ [here](https://bsse.ethz.ch/cevo/research/sars-cov-2/real-time-monitoring-in-switzerland.html), and an example for advanced visualization [here](https://opensource.com/article/20/4/python-data-covid-19). Feel free to share your conclusions and opinions on it on the forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You now know more about Transformers networks using Keras: this allows you to tackle several Natural Language Processing tasks, which are highly marketable at this time.\n",
    "- You should now have a deeper understanding of convolutions, especially on how things that appear small and easy (such as padding and striding) can lead to quite complex changes of behavior. For example, can we apply \"same\" padding with an filter of an even shape (e.g. 4 x 4, 6 x 6 etc.)? Would it be possible to pad the input such that, using a stride > 1, we get a matrix with the same shape as the input? This reasoning is important because these \"sizes\" in the network are hyperparameters, which means that you are responsible to set them correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
